# Plan: Custom DuckDB `duck_access` C++ Extension

## Goal

Build a C++ DuckDB extension (`duck_access`) that lets users query data from their **local DuckDB** (CLI, Python, etc.) while the existing Go API server remains the security control plane. Users run `SELECT * FROM titanic` and the extension transparently resolves it to presigned Parquet URLs with RLS filters and column masks applied.

## Architecture

```
Local DuckDB + duck_access extension
  │
  │  POST /v1/manifest  (X-API-Key header)
  ▼
Go API Server (existing)
  ├─ Auth (API Key → principal)
  ├─ RBAC check (SELECT privilege)
  ├─ RLS row filters (from SQLite)
  ├─ Column masks (from SQLite)
  ├─ DuckLake SQLite → Parquet file paths
  └─ Presign S3 URLs (AWS SDK v2, Hetzner-compatible)
      │
      ▼
Extension rewrites query to:
  SELECT <masked_cols> FROM read_parquet([presigned_urls])
  WHERE <rls_filters>
```

No raw S3 credentials leave the server. Users only receive presigned URLs scoped to the exact files they're allowed to see.

---

## Part A: Go API Changes

### A1. Add `POST /v1/manifest` endpoint

**Files to modify:**
- `internal/api/openapi.yaml` — add ManifestRequest/Response schemas and `/manifest` path
- `internal/api/handler.go` — add `GetManifest` handler method
- Regenerate `server.gen.go` and `types.gen.go` via `oapi-codegen`

**Request:**
```json
{ "table": "titanic", "schema": "main" }
```
+ `X-API-Key` header (existing auth middleware)

**Response (200):**
```json
{
  "table": "titanic",
  "schema": "main",
  "columns": [{"name": "PassengerId", "type": "BIGINT"}, ...],
  "files": ["https://hel1.your-objectstorage.com/bucket/lake_data/...?X-Amz-Signature=..."],
  "row_filters": ["\"Pclass\" = 1"],
  "column_masks": {"Name": "'***'"},
  "expires_at": "2026-02-10T12:30:00Z"
}
```

**Error responses:** 401 (unauthorized), 403 (access denied), 404 (table not found)

### A2. DuckLake file resolution

Query the DuckLake SQLite metastore to map table → Parquet files:

```sql
-- Get data_path prefix
SELECT value FROM ducklake_metadata WHERE key = 'data_path' AND scope IS NULL;

-- Get active data files for a table
SELECT path, path_is_relative FROM ducklake_data_file
WHERE table_id = ? AND end_snapshot IS NULL;
```

For relative paths: full path = `{data_path}{path}` (e.g., `s3://yacobolo/lake_data/xxx.parquet`)

**New file:** `internal/service/manifest.go` — `ManifestService.GetManifest()` that:
1. Calls existing `LookupTableID` for table resolution
2. Calls existing `CheckPrivilege` for RBAC
3. Calls existing `GetEffectiveRowFilters` for RLS
4. Calls existing `GetEffectiveColumnMasks` for column masks
5. Calls existing `ListColumns` for schema info
6. Queries `ducklake_data_file` for Parquet paths (new query)
7. Generates presigned URLs via new `S3Presigner`

### A3. Presigned URL generation

**New file:** `internal/service/presigner.go`

**New dependency:** `github.com/aws/aws-sdk-go-v2` (+ `/config`, `/credentials`, `/service/s3`)

```go
s3Client := s3.New(s3.Options{
    Region:       cfg.S3Region,
    Credentials:  credentials.NewStaticCredentialsProvider(cfg.S3KeyID, cfg.S3Secret, ""),
    BaseEndpoint: aws.String("https://" + cfg.S3Endpoint),
    UsePathStyle: true,  // Required for Hetzner S3
})
presignClient := s3.NewPresignClient(s3Client)
```

Parse `s3://bucket/key` from DuckLake paths → call `presignClient.PresignGetObject()` with 15-minute expiry.

### A4. Wire into `cmd/server/main.go`

- Create `S3Presigner` from config
- Create `ManifestService` with existing services + presigner
- Pass to handler

---

## Part B: C++ Extension

### B1. Repository setup

Clone `https://github.com/duckdb/extension-template` into a new directory (e.g., `extension/duck_access/` or a separate repo). Pin to the DuckDB version matching your Go bindings.

```
duck_access/
├── src/
│   ├── duck_access_extension.cpp   # Load() entry point
│   ├── duck_access_secret.cpp/hpp  # Secret type registration
│   ├── duck_access_scan.cpp/hpp    # Replacement scan
│   ├── duck_access_http.cpp/hpp    # HTTP client (cpp-httplib)
│   ├── duck_access_manifest.cpp/hpp # JSON parsing + cache
│   └── include/
│       ├── httplib.h               # Vendored cpp-httplib
│       └── json.hpp                # Vendored nlohmann/json
├── test/sql/duck_access.test
├── CMakeLists.txt
├── Makefile
└── extension_config.cmake
```

### B2. Secret type: `duck_access`

Register via `ExtensionUtil::RegisterSecretType` + `ExtensionUtil::RegisterFunction`:
- Type name: `duck_access`
- Parameters: `API_URL` (varchar, required), `API_KEY` (varchar, required, redacted), `SCOPE` (varchar, optional)
- Provider: `config` (user provides values directly)

**User experience:**
```sql
LOAD duck_access;
CREATE SECRET my_platform (
    TYPE duck_access,
    API_URL 'https://api.example.com/v1',
    API_KEY 'key_abc123'
);
```

API_KEY is marked as redacted so `SELECT * FROM duckdb_secrets()` won't expose it.

For multi-server setups, users can scope secrets:
```sql
CREATE SECRET prod (TYPE duck_access, API_URL 'https://prod.example.com/v1', API_KEY '...', SCOPE 'prod');
CREATE SECRET staging (TYPE duck_access, API_URL 'https://staging.example.com/v1', API_KEY '...', SCOPE 'staging');
```

Since API keys don't expire (unlike JWT tokens), no automatic refresh logic is needed in the initial implementation. If JWT-based auth is added later, a custom Secret Provider callback can handle token exchange/refresh transparently.

### B3. Replacement scan

Register via `config.replacement_scans.emplace_back(callback, nullptr)`.

**Flow when user runs `SELECT * FROM titanic`:**

1. DuckDB can't find `titanic` locally → calls replacement scan
2. Check if a `duck_access` secret exists. If not → return `nullptr` (normal "table not found" error)
3. HTTP POST to `{api_url}/manifest` with `{"table": "titanic", "schema": "main"}` and `X-API-Key` header
4. Parse JSON response into `TableManifest` struct
5. Build AST: `SubqueryRef` wrapping a `SelectNode`:
   - FROM: `read_parquet([presigned_url1, presigned_url2, ...], SECRET='NONE')` — SECRET='NONE' prevents httpfs from injecting S3 auth headers onto presigned URLs
   - SELECT: explicit column list with mask expressions applied (e.g., `'***' AS "Name"`)
   - WHERE: parsed RLS filter expressions combined with AND
6. Return the `SubqueryRef` aliased as the table name

DuckDB then merges the user's original WHERE clause on top of the subquery, making RLS un-bypassable.

**JOINs:** Replacement scan is called once per unresolved table name. No special handling needed — DuckDB handles this naturally.

### B4. HTTP client

Vendor **cpp-httplib** (`httplib.h`) with `CPPHTTPLIB_OPENSSL_SUPPORT` for HTTPS. DuckDB already links OpenSSL.

Wrapper: `DuckAccessHttp::PostJson(url, api_key, json_body)` → returns `{status_code, body, error}`.

### B5. Manifest cache

In-memory cache keyed by `"schema.table"`. Entries expire based on `expires_at` from the API response minus a 60-second safety margin. On cache miss or expiry, refetch from API.

### B6. Build

CMakeLists.txt extends the extension template, adds OpenSSL linkage for cpp-httplib HTTPS support. `make` produces `duck_access.duckdb_extension`.

---

## Part C: Edge Cases & Technical Risks

| Case | Handling |
|------|----------|
| Table exists locally | Replacement scan not called (DuckDB resolves it) |
| Auth fails (401) | Throw `BinderException("duck_access: authentication failed")` |
| Access denied (403) | Throw `BinderException("duck_access: access denied on table X")` |
| API unreachable | Throw `BinderException("duck_access: cannot reach API at {url}")` with 10s timeout |
| No secret configured | Return `nullptr` from scan → normal DuckDB "table not found" |
| Empty table (no files) | Return empty result with correct schema |
| Presigned URL expiry during long query | Set server-side expiry to 1 hour; document limitation |
| Schema-qualified names (`main.titanic`) | Pass schema to manifest API; default to `main` |

### Critical: httpfs S3 signature conflict

When `read_parquet` is called with presigned HTTPS URLs, DuckDB's `httpfs` extension may try to match a stored S3 secret by hostname and add `Authorization` headers. Standard S3 behavior dictates that a request cannot have both a presigned signature (in the query string) and an Authorization header — this will cause HTTP 403 from Hetzner.

**Mitigation (three layers):**

1. The Go API returns presigned URLs as plain `https://` URLs (not `s3://`) — this is the default behavior of `PresignGetObject`.
2. **In the replacement scan, pass `SECRET 'NONE'` as a named parameter to the `read_parquet` call.** This explicitly tells httpfs to skip credential injection for these specific URLs, regardless of what other secrets the user has configured. In the C++ AST construction:
   ```cpp
   // After building the file list argument, add SECRET parameter
   auto secret_param = make_uniq<ConstantExpression>(Value("NONE"));
   // Add as named parameter "secret" to the FunctionExpression
   function->named_parameters["secret"] = std::move(secret_param);
   ```
3. As a fallback, if a user has a Hetzner S3 secret with a matching endpoint, the `SECRET 'NONE'` parameter takes precedence and prevents httpfs from interfering.

### RLS filter syntax compatibility

The Go API stores RLS filters as raw SQL expressions (e.g., `"Pclass" = 1`). These are parsed by `pg_query_go` (PostgreSQL dialect). DuckDB's `Parser::ParseExpressionList` uses DuckDB's own parser. Most simple expressions (`col = val`, `col IN (...)`, `col BETWEEN x AND y`) parse identically, but edge cases may differ.

**Mitigation:** Ensure RLS filter expressions use DuckDB-compatible SQL syntax. Since the Go API already executes these filters against DuckDB (via the existing query engine), the filters are already known to be DuckDB-compatible.

### Future: DuckDB-Wasm support

The C++ extension will **not** work in DuckDB-Wasm without changes:
- cpp-httplib uses raw sockets; Wasm requires Emscripten Fetch API or JS bridge
- Wasm extensions need separate compilation with Emscripten toolchain
- CORS must be configured on both the Go API and Hetzner S3 bucket

This is out of scope for the initial implementation but should be planned as a follow-up phase.

---

## Implementation Order

### Phase 1: Go API — Manifest endpoint (est. 1-2 days)
1. Add `S3Presigner` (`internal/service/presigner.go`) + AWS SDK v2 dependency
2. Add DuckLake file resolution query (`ducklake_data_file`)
3. Add `ManifestService` (`internal/service/manifest.go`)
4. Add OpenAPI spec entries for `/manifest`
5. Regenerate oapi-codegen types/server
6. Add handler + wire in `main.go`
7. Test with `curl`

### Phase 2: C++ Extension — Skeleton + secrets (est. 1 day)
1. Clone extension-template, rename to `duck_access`
2. Implement secret type + provider registration
3. Verify `CREATE SECRET` works in DuckDB CLI

### Phase 3: C++ Extension — HTTP + manifest (est. 1 day)
1. Vendor cpp-httplib and nlohmann/json
2. Implement HTTP POST client
3. Implement manifest JSON parser + cache

### Phase 4: C++ Extension — Replacement scan (est. 2-3 days)
1. Basic scan: files only (no filters/masks) → `read_parquet([urls])`
2. Add row filter injection via `Parser::ParseExpressionList` → WHERE clause
3. Add column mask injection → SELECT list rewriting with SubqueryRef
4. Error handling for all edge cases

### Phase 5: Automated Integration Test Suite (NEXT — ready to implement)

Replaces all manual testing with a Go integration test suite that exercises the full end-to-end flow: DuckDB CLI → duck_access extension → Go API server → S3 presigned URLs.

#### Files to create/modify:
1. **`test/integration/helpers_test.go`** — shared test infrastructure
2. **`test/integration/extension_test.go`** — 9 test cases
3. **`Taskfile.yml`** — add `integration-test` task

#### Architecture:
```
┌────────────────────┐    HTTP (X-API-Key)     ┌──────────────────────┐
│  DuckDB CLI v1.4.4 │ ─────────────────────> │  httptest.Server      │
│  + duck_access ext  │ <───────────────────── │  (in-process Go)      │
│  (subprocess/test)  │   JSON manifest resp   │                      │
│                     │                         │  AuthMiddleware       │
│  read_parquet(url)  │ ── presigned S3 GET ─> │  ManifestService     │
│  → query results    │                         │  S3Presigner (real)  │
└────────────────────┘                         │  RBAC + DuckLake meta│
                                                └──────────────────────┘
```

#### Prerequisites (auto-skipped if missing):
- Extension binary: `extension/duck_access/build/release/extension/duck_access/duck_access.duckdb_extension`
- DuckDB CLI v1.4.4: `extension/duck_access/build/release/duckdb` (NOT Homebrew v1.4.1)
- S3 credentials: `.env` with `KEY_ID`, `SECRET`, `ENDPOINT`, `REGION`
- DuckLake metadata: `ducklake_meta.sqlite` (for table/column/file resolution — but we seed this directly)

#### Step 1: `test/integration/helpers_test.go`

**Build tag:** `//go:build integration` — excluded from `go test ./...`

**Path helpers** (derive from `runtime.Caller` → project root):
- `projectRoot() string`
- `extensionPath() string` → absolute path to `.duckdb_extension`
- `duckdbCLIPath() string` → absolute path to built DuckDB v1.4.4 CLI
- `dotEnvPath() string` → absolute path to `.env`

**`checkPrerequisites(t)`** — skip if binaries/creds missing:
- Stat extension binary, DuckDB CLI binary
- Load `.env`, check `KEY_ID`, `SECRET`, `ENDPOINT`, `REGION` env vars

**`sha256Hex(s string) string`** — compute SHA-256 for API key storage

**`seedDuckLakeMetadata(t, db)`** — creates DuckLake tables and inserts hardcoded production data:
- `ducklake_metadata`: `data_path = 's3://yacobolo/lake_data/'`
- `ducklake_schema`: schema_id=0, name='main'
- `ducklake_table`: table_id=1, name='titanic', schema_id=0
- `ducklake_column`: all 12 titanic columns (PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked) with correct `column_type` values
- `ducklake_data_file`: `ducklake-019c4727-c55c-7e4d-ab38-e01a2920253c.parquet`, path_is_relative=1, record_count=891

Rationale: Hardcode exact production values rather than ATTACH to `ducklake_meta.sqlite`. Tests are self-documenting, deterministic, and don't risk mutating production data.

**`seedRBAC(t, db) apiKeys`** — returns struct `{Admin, Analyst, Researcher string}`:
- 3 principals: `admin_user` (is_admin=1), `analyst1`, `researcher1`
- 3 groups: `admins`, `analysts`, `researchers` with correct memberships
- Grants: admins→ALL_PRIVILEGES on catalog; analysts/researchers→USAGE on schema + SELECT on table
- Row filter: analysts→`"Pclass" = 1` on table 1
- Column mask: Name→`'***'` on table 1; analysts see_original=0, researchers see_original=1
- API keys: `test-admin-key`, `test-analyst-key`, `test-researcher-key` (stored as SHA-256 hashes via `dbstore.CreateAPIKey`)

**`setupIntegrationServer(t) *testEnv`** — returns `{Server *httptest.Server, Keys apiKeys}`:
1. Load `.env` → `config.LoadFromEnv()`
2. Create temp SQLite with `_foreign_keys=on`
3. Run migrations via `internaldb.RunMigrations`
4. Call `seedDuckLakeMetadata` + `seedRBAC`
5. Build all repos: principal, group, grant, rowFilter, columnMask, audit, introspection, apiKey
6. Build `AuthorizationService`, `S3Presigner` (real, from config), `ManifestService`
7. Build remaining services (querySvc with nil engine — never hit `/query`)
8. Create `api.NewHandler(...)` with all 9 params including manifest
9. Create chi router with `middleware.AuthMiddleware(jwtSecret, apiKeyRepo)` (REAL auth)
10. Mount handlers under `/v1` prefix via `r.Route("/v1", ...)`
11. Return `httptest.NewServer(r)`

**`runDuckDBQuery(t, serverURL, apiKey, sql) (duckDBResult, stderr, error)`**:
- Builds SQL preamble: `SET allow_unsigned_extensions = true; LOAD '<ext_path>'; CREATE SECRET ...; <query>`
- Runs: `<duckdb_cli_path> -json` with SQL piped via stdin
- 30-second timeout via `context.WithTimeout`
- Parses stdout as `[]map[string]interface{}` (DuckDB `-json` outputs `[{"col":"val"}, ...]`)
- Returns parsed result + stderr string + error

**Assertion helpers:**
- `getScalarInt(t, result, column) int` — extract single int from 1-row result (handles float64→int from JSON)
- `getColumnValues(result, column) []interface{}` — extract all values for a column

#### Step 2: `test/integration/extension_test.go`

**Single parent test `TestExtension`** calls `checkPrerequisites` + `setupIntegrationServer` once, then runs 9 parallel subtests (each spawns fresh DuckDB CLI process):

| Subtest | API Key | SQL | Assertion |
|---------|---------|-----|-----------|
| `Admin_FullAccess` | admin | `SELECT * FROM titanic LIMIT 5` | 5 rows, Name ≠ '***' |
| `Admin_RowCount_891` | admin | `SELECT count(*) as cnt FROM titanic` | cnt = 891 |
| `Analyst_RLS_Pclass1` | analyst | `SELECT DISTINCT "Pclass" FROM titanic` | only value 1 |
| `Analyst_RowCount_216` | analyst | `SELECT count(*) as cnt FROM titanic` | cnt = 216 |
| `Analyst_NameMasked` | analyst | `SELECT DISTINCT "Name" FROM titanic LIMIT 1` | = '***' |
| `Researcher_AllRows` | researcher | `SELECT count(*) as cnt FROM titanic` | cnt = 891 |
| `Researcher_NameVisible` | researcher | `SELECT "Name" FROM titanic LIMIT 1` | ≠ '***' |
| `InvalidAPIKey_Error` | `invalid-key` | `SELECT * FROM titanic LIMIT 1` | err ≠ nil, stderr contains "401"/"unauthorized" |
| `NonexistentTable_Error` | admin | `SELECT * FROM nonexistent_table LIMIT 1` | err ≠ nil, stderr contains "not found"/"404" |

Subtests use `t.Parallel()` — safe because each spawns its own DuckDB CLI subprocess and the shared httptest.Server is thread-safe.

#### Step 3: `Taskfile.yml` update

Add task:
```yaml
  integration-test:
    desc: Run integration tests (requires extension build + S3 credentials)
    cmds:
      - go test -tags integration -v -timeout 120s ./test/integration/...
```

#### Edge cases handled:
1. **HTTPS vs httpfs**: Presigned URLs are plain `https://`, no `httpfs` needed — DuckDB handles natively
2. **JSON number parsing**: Go's `json.Unmarshal` with `interface{}` gives `float64` — `getScalarInt` converts
3. **DuckDB CLI version**: Must use built v1.4.4 CLI, not Homebrew v1.4.1 (version mismatch would fail extension load)
4. **Secret name collision**: Each subtest is a fresh DuckDB process — no collision even with `t.Parallel()`
5. **Column name case**: Titanic uses PascalCase (PassengerId, Name, Pclass) — tests use exact case
6. **nil engine**: `NewQueryService(nil, auditRepo)` is safe — integration tests never hit `/v1/query`
7. **Error output location**: DuckDB writes errors to stderr; `-json` only affects stdout

#### Run commands:
```bash
task integration-test                                              # via Taskfile
go test -tags integration -v -timeout 120s ./test/integration/...  # directly
go test -tags integration -v -run TestExtension/Admin_RowCount_891 ./test/integration/...  # single test
```

**Total: ~6-9 days** (Phases 1-4 complete, Phase 5 done)

---

### Phase 5b: Integration Test Improvements (NEXT — ready to implement)

Refactor to table-driven tests, fix a latent JSON parser bug, and add 3 new test cases.

#### Files to modify:
- `test/integration/helpers_test.go`
- `test/integration/extension_test.go`

#### Change 1: Fix `extractLastJSONArray` bracket bug

Current implementation counts raw `[` and `]` characters. If a JSON string value contains brackets (e.g. `"tags": "[a,b]"`), the parser breaks. Fix: skip characters inside JSON strings (track `"` toggling, respecting `\"`).

#### Change 2: Refactor to table-driven tests

Replace 9 separate `t.Run` closures with a single `[]struct` test table. Each entry defines:

```go
type testCase struct {
    name     string
    apiKey   string        // which key to use (field name on apiKeys, or literal for invalid)
    sql      string        // query to run
    wantErr  bool          // expect DuckDB CLI to exit non-zero
    check    func(t *testing.T, result duckDBResult, stderr string)
}
```

The loop:
```go
for _, tc := range cases {
    t.Run(tc.name, func(t *testing.T) {
        t.Parallel()
        result, stderr, err := runDuckDBQuery(t, env.Server.URL, tc.apiKey, tc.sql)
        if tc.wantErr {
            if err == nil { t.Fatal("expected error, got success") }
        } else {
            if err != nil { t.Fatalf("query failed: %v\nstderr: %s", err, stderr) }
        }
        tc.check(t, result, stderr)
    })
}
```

This eliminates the duplicated error-check boilerplate from every subtest.

#### Change 3: Add `NoAccess_403` test

Seed a 4th principal `no_access_user` with an API key but **no grants**. Query `SELECT * FROM titanic LIMIT 1` — should fail with "access denied" or "403" in stderr.

Requires changes to `helpers_test.go`:
- Add `NoAccess string` field to `apiKeys` struct
- Create `no_access_user` principal (is_admin=0) in `seedRBAC`
- Create API key `test-noaccess-key` for this user
- No group membership, no grants

#### Change 4: Verify all 12 columns in `Admin_FullAccess`

Add assertion checking that the first result row contains all 12 expected column keys: `PassengerId`, `Survived`, `Pclass`, `Name`, `Sex`, `Age`, `SibSp`, `Parch`, `Ticket`, `Fare`, `Cabin`, `Embarked`.

#### Change 5: Add `Analyst_RLS_WithUserWhere` test

Analyst runs `SELECT count(*) as cnt FROM titanic WHERE "Age" > 30`. The RLS filter `"Pclass" = 1` must still apply on top of the user's WHERE. Assert `cnt < 216` (fewer than all Pclass=1 rows, since Age>30 filters further). This validates RLS can't be bypassed by user-supplied WHERE clauses.

#### Change 6: Add `Admin_AuditLogWritten` test

After a successful admin query, hit the Go API's audit log endpoint (`GET /v1/audit-logs`) via HTTP to verify a MANIFEST audit entry was recorded. This tests the audit side-effect of the manifest endpoint.

Approach: After `runDuckDBQuery` succeeds, make an HTTP GET to `env.Server.URL + "/v1/audit-logs"` with the admin API key header. Parse JSON response. Assert at least one entry with `action == "MANIFEST"` and `status == "ALLOWED"`.

Note: This hits the Go API directly (not via DuckDB CLI), which is fine — it's testing a server-side side-effect.

#### Change 7: Clean up `Analyst_NameMasked`

Change from `SELECT DISTINCT "Name" FROM titanic LIMIT 1` to `SELECT DISTINCT "Name" FROM titanic`. Since all names are masked to `'***'`, DISTINCT returns exactly 1 row. Assert `len(result) == 1` AND `result[0]["Name"] == "***"`. Clearer intent.

#### Final test table (12 cases):

| # | Name | Key | SQL | Assertion |
|---|------|-----|-----|-----------|
| 1 | `Admin_FullAccess` | admin | `SELECT * FROM titanic LIMIT 5` | 5 rows, all 12 columns present, Name ≠ '***' |
| 2 | `Admin_RowCount_891` | admin | `SELECT count(*) as cnt FROM titanic` | cnt = 891 |
| 3 | `Analyst_RLS_Pclass1` | analyst | `SELECT DISTINCT "Pclass" FROM titanic` | 1 row, value = 1 |
| 4 | `Analyst_RowCount_216` | analyst | `SELECT count(*) as cnt FROM titanic` | cnt = 216 |
| 5 | `Analyst_NameMasked` | analyst | `SELECT DISTINCT "Name" FROM titanic` | 1 row, value = '***' |
| 6 | `Analyst_RLS_WithUserWhere` | analyst | `SELECT count(*) as cnt FROM titanic WHERE "Age" > 30` | cnt > 0 AND cnt < 216 |
| 7 | `Researcher_AllRows` | researcher | `SELECT count(*) as cnt FROM titanic` | cnt = 891 |
| 8 | `Researcher_NameVisible` | researcher | `SELECT "Name" FROM titanic LIMIT 1` | Name ≠ '***' |
| 9 | `NoAccess_403` | noaccess | `SELECT * FROM titanic LIMIT 1` | wantErr, stderr contains "access denied"/"403" |
| 10 | `InvalidAPIKey_401` | `invalid-key-12345` | `SELECT * FROM titanic LIMIT 1` | wantErr, stderr contains "401"/"unauthorized" |
| 11 | `NonexistentTable_404` | admin | `SELECT * FROM nonexistent_table LIMIT 1` | wantErr, stderr contains "not found"/"404" |
| 12 | `Admin_AuditLogWritten` | admin | `SELECT count(*) as cnt FROM titanic` | cnt = 891, then HTTP GET audit-logs has MANIFEST entry |

#### Verification:
```bash
task integration-test   # all 12 tests should pass
```

---

## Verification

1. **Integration tests:** `task integration-test` — runs all 12 automated tests
2. **Manual smoke test:** `curl -X POST http://localhost:8080/v1/manifest -H "X-API-Key: <key>" -d '{"table":"titanic"}' | jq`
3. **Existing tests:** `go test ./...` — verify no regressions (handler_test.go needs 9th param fix)

---

## Files to Create/Modify

**Integration tests (modify):**
- `test/integration/helpers_test.go` — fix JSON parser, add NoAccess user, add audit helper
- `test/integration/extension_test.go` — refactor to table-driven, add 3 new cases

**Known pre-existing issue (out of scope, not caused by this change):**
- `internal/api/handler_test.go` line 130 — calls `NewHandler` with 8 params, needs 10th. This test was broken before our changes. Not part of this plan.

---

## Phase 6: Speed Up Catalog Integration Tests

### Problem

The 6 `TestCatalog_*` functions each call `setupLocalDuckLake(t)` independently, creating:
- 6 separate in-memory DuckDB instances
- 24 extension install/load operations (`INSTALL ducklake`, `LOAD ducklake`, `INSTALL sqlite`, `LOAD sqlite` × 6)
- 54 migration runs (9 migrations × 6 tests)
- 6 temp SQLite metastores

Total catalog test time: ~1.1s. The tests themselves run in <50ms — setup dominates.

Extension tests (~19s) are left as-is. They must spawn real DuckDB CLI processes and hit real S3, so the bottleneck is I/O-bound and can't be reduced without mocking S3.

### Solution: `TestMain` + shared DuckLake instance + `t.Parallel()`

Use `TestMain` to create **one** DuckLake instance shared by all 6 catalog test functions. Each test function gets its own `CatalogRepo` pointing at the shared `(duckDB, metaDB)` pair. Tests that mutate state use unique prefixed names to avoid collisions (they already do — `analytics`, `test_schema`, `cascade_schema`, etc.).

Add `t.Parallel()` to all 6 `TestCatalog_*` functions so they run concurrently with each other.

**Expected speedup:** ~1.1s → ~0.3s for catalog tests (1 setup instead of 6, plus parallelism).

### Files to modify

**`test/integration/helpers_test.go`:**

1. Add a package-level `var sharedCatalogEnv *catalogTestEnv` variable
2. Add `func TestMain(m *testing.M)` that:
   - Calls `setupSharedDuckLake()` (non-test version of `setupLocalDuckLake` that doesn't take `*testing.T`)
   - Runs `m.Run()`
   - Cleans up (close DB connections, remove temp dir)
3. Add `func setupSharedDuckLake() (*catalogTestEnv, func(), error)`:
   - Same logic as `setupLocalDuckLake` but returns cleanup func + error instead of calling `t.Skip`/`t.Fatal`
   - On failure, sets `sharedCatalogEnv = nil` (tests will skip)
4. Add `func requireCatalogEnv(t *testing.T) *catalogTestEnv`:
   - If `sharedCatalogEnv == nil`, calls `t.Skip("DuckLake setup failed or unavailable")`
   - Otherwise returns the shared env
5. Keep existing `setupLocalDuckLake(t)` as-is (it's still useful if someone wants an isolated instance)

**`test/integration/catalog_test.go`:**

1. Replace `setupLocalDuckLake(t)` calls with `requireCatalogEnv(t)` in all 6 test functions
2. Add `t.Parallel()` at the top of each `TestCatalog_*` function
3. Ensure test data names are already unique across tests (they are: `analytics`, `test_schema`, `existing_schema`, `cascade_schema`, `summary_schema`, `page_alpha/bravo/charlie/delta/echo`)

### Detailed implementation

```go
// helpers_test.go — new additions

var sharedCatalogEnv *catalogTestEnv
var sharedCatalogCleanup func()

func TestMain(m *testing.M) {
    env, cleanup, err := setupSharedDuckLake()
    if err != nil {
        // DuckLake not available — catalog tests will skip, extension tests unaffected
        fmt.Fprintf(os.Stderr, "DuckLake setup skipped: %v\n", err)
    } else {
        sharedCatalogEnv = env
        sharedCatalogCleanup = cleanup
    }

    code := m.Run()

    if sharedCatalogCleanup != nil {
        sharedCatalogCleanup()
    }
    os.Exit(code)
}

func setupSharedDuckLake() (*catalogTestEnv, func(), error) {
    tmpDir, err := os.MkdirTemp("", "ducklake-integration-*")
    if err != nil {
        return nil, nil, err
    }
    metaPath := filepath.Join(tmpDir, "meta.sqlite")
    dataPath := filepath.Join(tmpDir, "lake_data") + "/"
    os.MkdirAll(dataPath, 0o755)

    duckDB, err := sql.Open("duckdb", "")
    if err != nil {
        os.RemoveAll(tmpDir)
        return nil, nil, err
    }

    for _, stmt := range []string{
        "INSTALL ducklake", "LOAD ducklake",
        "INSTALL sqlite", "LOAD sqlite",
    } {
        if _, err := duckDB.Exec(stmt); err != nil {
            duckDB.Close()
            os.RemoveAll(tmpDir)
            return nil, nil, fmt.Errorf("%s: %w", stmt, err)
        }
    }

    attachSQL := fmt.Sprintf(
        `ATTACH 'ducklake:sqlite:%s' AS lake (DATA_PATH '%s')`,
        metaPath, dataPath,
    )
    if _, err := duckDB.Exec(attachSQL); err != nil {
        duckDB.Close()
        os.RemoveAll(tmpDir)
        return nil, nil, fmt.Errorf("attach: %w", err)
    }
    if _, err := duckDB.Exec("USE lake"); err != nil {
        duckDB.Close()
        os.RemoveAll(tmpDir)
        return nil, nil, fmt.Errorf("use lake: %w", err)
    }

    metaDB, err := sql.Open("sqlite3", metaPath+"?_foreign_keys=on")
    if err != nil {
        duckDB.Close()
        os.RemoveAll(tmpDir)
        return nil, nil, err
    }

    if err := internaldb.RunMigrations(metaDB); err != nil {
        metaDB.Close()
        duckDB.Close()
        os.RemoveAll(tmpDir)
        return nil, nil, fmt.Errorf("migrations: %w", err)
    }

    cleanup := func() {
        metaDB.Close()
        duckDB.Close()
        os.RemoveAll(tmpDir)
    }

    return &catalogTestEnv{DuckDB: duckDB, MetaDB: metaDB}, cleanup, nil
}

func requireCatalogEnv(t *testing.T) *catalogTestEnv {
    t.Helper()
    if sharedCatalogEnv == nil {
        t.Skip("DuckLake extensions not available — skipping catalog test")
    }
    return sharedCatalogEnv
}
```

```go
// catalog_test.go — changes in each test function

func TestCatalog_SchemaCRUD(t *testing.T) {
    t.Parallel()
    env := requireCatalogEnv(t)    // was: setupLocalDuckLake(t)
    repo := repository.NewCatalogRepo(env.MetaDB, env.DuckDB)
    // ... rest unchanged
}
// Same pattern for all 6 functions
```

### Concurrency safety

DuckDB supports concurrent reads. DDL operations (CREATE/DROP SCHEMA/TABLE) serialize internally via DuckDB's write lock. The shared `metaDB` SQLite connection also serializes writes. This is safe because:

1. Each test uses uniquely-named schemas/tables (no name collisions)
2. DuckDB's internal locking handles concurrent DDL correctly
3. SQLite in WAL mode (default for mattn/go-sqlite3) supports concurrent reads + serialized writes
4. The `CatalogRepo` read-back after DDL (`GetSchema`) is a point query on the specific schema name, so it won't be affected by concurrent creates of other schemas

### Verification

```bash
# Run only catalog tests — should be ~0.3s instead of ~1.1s
go test -tags integration -v -timeout 120s -run 'TestCatalog' ./test/integration/ -count=1

# Run all integration tests — extension tests unaffected
go test -tags integration -v -timeout 120s ./test/integration/ -count=1
```

# Plan: Integrate DuckLake with Hetzner S3 + SQLite Metastore + RBAC/RLS

## Goal

Replace the local `titanic.parquet` file with a **DuckLake lakehouse** backed by:
- **SQLite** as the metadata catalog (lightweight, file-based, supports multi-client)
- **Hetzner Object Storage** (S3-compatible) as the data storage layer

Then wrap our existing Substrait-based RBAC + RLS framework around DuckLake-attached tables, creating a scalable data platform with row-level security.

## Background & Key Findings

- **Current setup**: `titanic.parquet` loaded into an in-memory DuckDB table via `CREATE TABLE titanic AS SELECT * FROM 'titanic.parquet'`
- **Go driver**: Currently using `marcboeker/go-duckdb` v1.8.5 (bundles DuckDB v1.1.3). This driver is **archived** and migrated to `duckdb/duckdb-go`. DuckLake requires DuckDB >= 1.3.0, so we **must upgrade** to `duckdb/duckdb-go` v2.x (latest v2.5.5, bundles DuckDB v1.4.4).
- **Substrait extension**: Now a community extension (`INSTALL substrait FROM community`). Should work with DuckLake-attached tables since they appear as regular named tables in the catalog.
- **DuckLake tables in Substrait plans**: Tables will appear as compound `NamedTable` references like `["lake", "main", "titanic"]`. Our walker already handles compound names via `resolveTableName()` (uses last element). Policies should match on the table name portion only.
- **Hetzner credentials**: `.env` has `KEY_ID`, `SECRET`, `ENDPOINT=hel1.your-objectstorage.com`, `REGION=hel1`. Bucket name: `duck-demo`.

## Risk: Substrait + DuckLake Compatibility

This is the **#1 unknown**. No documentation confirms that `get_substrait()` / `from_substrait()` works with DuckLake-attached tables. The approach:
1. First, validate this works manually in a spike before committing to the full integration
2. If it works: proceed with full plan
3. If it doesn't: fall back to direct S3 parquet reads (`SELECT * FROM 's3://...'`) registered as tables

---

## Phase 1: Upgrade Go Driver [CRITICAL] — Size: M

**Problem**: `marcboeker/go-duckdb` v1.8.5 bundles DuckDB v1.1.3. DuckLake needs >= 1.3.0.

**Changes:**
- `go.mod` — Replace `github.com/marcboeker/go-duckdb v1.8.5` with `github.com/duckdb/duckdb-go/v2 v2.5.5`
- All `.go` files — Rewrite import `_ "github.com/marcboeker/go-duckdb"` → `_ "github.com/duckdb/duckdb-go/v2"`
- Check for any API differences (v2 may have breaking changes around JSON scanning, etc.)
- Run `go mod tidy` to clean up dependencies
- Run existing test suite to confirm nothing is broken

**Files to modify:**
- `go.mod`
- `main.go` (import)
- `engine/engine_test.go` (import)

---

## Phase 2: Add .env Loading + Config [HIGH] — Size: S

**Problem**: `.env` file exists with Hetzner credentials but nothing loads it.

**Changes:**
- Add `github.com/joho/godotenv` dependency (or just use `os.Getenv` and load `.env` manually)
- Create `config/config.go` with a `Config` struct:
  ```go
  type Config struct {
      S3KeyID    string
      S3Secret   string
      S3Endpoint string
      S3Region   string
      S3Bucket   string
      MetaDBPath string // path to SQLite metadata file, default "ducklake_meta.sqlite"
  }
  ```
- Load from env vars: `KEY_ID`, `SECRET`, `ENDPOINT`, `REGION`, with `BUCKET` defaulting to `duck-demo`
- Add `.env` to `.gitignore` (credentials are currently committed!)

**Files to create/modify:**
- `config/config.go` (new)
- `.gitignore` — add `.env`
- `go.mod` — add `godotenv` dependency

---

## Phase 3: DuckDB + DuckLake + Hetzner Setup [CRITICAL] — Size: L

**Problem**: Need to initialize DuckDB with DuckLake extension, S3 credentials, and attach the lake.

**Changes to `engine/engine.go`:**

Add a new `SetupDuckLake` function (or extend `NewSecureEngine`) that:

1. Installs and loads required extensions:
   ```sql
   INSTALL ducklake; LOAD ducklake;
   INSTALL sqlite;   LOAD sqlite;
   INSTALL httpfs;   LOAD httpfs;
   INSTALL substrait FROM community; LOAD substrait;
   ```

2. Creates S3 secret for Hetzner:
   ```sql
   CREATE SECRET hetzner_s3 (
       TYPE S3,
       KEY_ID '<from config>',
       SECRET '<from config>',
       ENDPOINT 'hel1.your-objectstorage.com',
       REGION 'hel1',
       URL_STYLE 'path'
   );
   ```

3. Attaches DuckLake with SQLite metastore + Hetzner data path:
   ```sql
   ATTACH 'ducklake:sqlite:ducklake_meta.sqlite' AS lake (
       DATA_PATH 's3://duck-demo/lake_data/'
   );
   USE lake;
   ```

4. On first run, loads `titanic.parquet` into the lake:
   ```sql
   CREATE TABLE IF NOT EXISTS lake.main.titanic AS SELECT * FROM 'titanic.parquet';
   ```
   On subsequent runs, the table already exists in the metadata (SQLite) and data (Hetzner).

**Files to modify:**
- `engine/engine.go` — Add `SetupDuckLake(ctx, db, cfg)` function
- `main.go` — Call setup, pass config, remove old `CREATE TABLE` from parquet

---

## Phase 4: Spike — Validate Substrait + DuckLake [CRITICAL] — Size: S

Before completing the integration, we need to verify that the round-trip works:
```
get_substrait('SELECT * FROM lake.main.titanic') → blob → from_substrait(blob)
```

**Approach**: Write a small integration test that:
1. Sets up DuckLake with a local DuckDB metastore (no S3, just local files — to isolate variables)
2. Creates a table in the DuckLake
3. Calls `get_substrait()` on a query against it
4. Unmarshals the plan, inspects `NamedTable.Names` (expect `["lake", "main", "titanic"]`)
5. Calls `from_substrait()` on the blob
6. Verifies results come back

If this fails, we fall back to: `CREATE TABLE titanic AS SELECT * FROM 's3://duck-demo/titanic.parquet'` (direct S3 read, registered as a plain table — guaranteed to work with Substrait).

**Files to create:**
- `engine/ducklake_test.go` — spike/integration test

---

## Phase 5: Update Walker/Rewriter for Compound Names [HIGH] — Size: S

**Problem**: DuckLake tables appear as `["lake", "main", "titanic"]` in Substrait plans. Our `resolveTableName()` already handles this (uses last element). But policy matching needs to be verified.

**Changes:**
- Verify `resolveTableName(["lake", "main", "titanic"])` returns `"titanic"` (already the case)
- Verify policies defined with table name `"titanic"` match against the extracted name
- Add integration test: query DuckLake-attached table with RBAC + RLS through `SecureEngine`

**Files to modify:**
- `engine/engine_test.go` — Add DuckLake integration tests
- Potentially `engine/walker.go` — if any adjustments needed

---

## Phase 6: Update main.go Demo [MEDIUM] — Size: S

**Changes to `main.go`:**
1. Load config from `.env`
2. Call `SetupDuckLake(ctx, db, cfg)` instead of manual `CREATE TABLE` from parquet
3. Update the demo query (may need to qualify table: `lake.main.titanic` or just `titanic` after `USE lake`)
4. Keep the same 4-role demo to show RBAC + RLS working on the lake

**Files to modify:**
- `main.go`

---

## Phase 7: Upload titanic.parquet to Hetzner [MEDIUM] — Size: S

Two approaches:
1. **DuckDB COPY TO** (user's preferred method):
   ```sql
   COPY (SELECT * FROM 'titanic.parquet') TO 's3://duck-demo/titanic.parquet' (FORMAT PARQUET);
   ```
   But with DuckLake, we don't manually upload — DuckLake manages the Parquet files on S3 automatically when we do `CREATE TABLE ... AS SELECT * FROM 'titanic.parquet'`.

2. **DuckLake automatic**: When we `CREATE TABLE lake.main.titanic AS SELECT * FROM 'titanic.parquet'`, DuckLake writes the Parquet files to `s3://duck-demo/lake_data/` automatically. This is the correct approach.

**No manual upload needed** — DuckLake handles it. The first run of `main.go` will:
- Read `titanic.parquet` locally
- Write data as Parquet files to Hetzner via DuckLake
- Store metadata in SQLite
- Subsequent runs just read from the lake

---

## Phase 8: Tests + Cleanup [MEDIUM] — Size: M

- Update existing integration tests to work with DuckLake (or keep a parallel set for local-only)
- Add DuckLake-specific integration tests:
  - `TestDuckLakeRBACAdminAccess`
  - `TestDuckLakeRLSFiltering`
  - `TestDuckLakeNoAccessDenied`
- Run full test suite: `go test -race ./...`
- Run `go vet ./...`
- Ensure `.env` is gitignored

**Files to modify:**
- `engine/engine_test.go`
- `engine/ducklake_test.go` (new)

---

## Execution Order

```
Phase 1 (driver upgrade) ──→ Phase 2 (config/.env) ──→ Phase 3 (DuckLake setup)
                                                            │
                                                            ▼
                                                     Phase 4 (spike: substrait+ducklake)
                                                            │
                                           ┌────────────────┤
                                           ▼                ▼
                                    Phase 5 (walker)   Phase 7 (data upload via DuckLake)
                                           │                │
                                           ▼                ▼
                                    Phase 6 (main.go demo)
                                           │
                                           ▼
                                    Phase 8 (tests + cleanup)
```

Phase 4 is the **critical gate**. If Substrait doesn't work with DuckLake, we pivot to direct S3 parquet reads.

---

## Fallback Plan (if Substrait + DuckLake fails)

If `get_substrait()` doesn't work with DuckLake-attached tables:

```sql
-- Instead of DuckLake ATTACH, use direct S3 reads:
CREATE SECRET hetzner_s3 (...);
CREATE TABLE titanic AS SELECT * FROM 's3://duck-demo/titanic.parquet';
```

This still gives us:
- Remote data on Hetzner (scalable)
- RBAC + RLS via Substrait (unchanged)
- Just no DuckLake features (ACID transactions, time travel, schema evolution)

We'd upload `titanic.parquet` to Hetzner using DuckDB's `COPY TO 's3://...'`.

---

## Files Summary

| File | Action | Phase |
|------|--------|-------|
| `go.mod` | Modify (upgrade driver, add godotenv) | 1, 2 |
| `main.go` | Modify (imports, config, DuckLake setup) | 1, 6 |
| `engine/engine.go` | Modify (imports, add SetupDuckLake) | 1, 3 |
| `engine/engine_test.go` | Modify (imports, DuckLake tests) | 1, 5, 8 |
| `config/config.go` | Create | 2 |
| `.gitignore` | Modify (add .env) | 2 |
| `engine/ducklake_test.go` | Create (spike + integration tests) | 4, 8 |
| `engine/walker.go` | Possibly modify | 5 |

---

## Verification

1. `go build ./...` — compiles with new driver
2. `go test -race ./...` — all existing + new tests pass
3. `go vet ./...` — clean
4. `go run main.go` — demo runs, queries DuckLake on Hetzner with RBAC/RLS
5. Verify SQLite metadata file created locally (`ducklake_meta.sqlite`)
6. Verify data exists on Hetzner (`s3://duck-demo/lake_data/` has Parquet files)
7. Second run of `main.go` works without re-uploading (reads from existing lake)
8. All 4 roles produce correct output (admin=all rows, first_class=Pclass=1, survivor=Survived=1, no_access=denied)

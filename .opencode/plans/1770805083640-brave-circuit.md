# Production Metadata Catalog — Phase 1 Plan

Bring the metadata catalog to production-grade parity with Unity Catalog / OpenMetadata. 12 items, ordered by dependency.

---

## Current Status & Remaining Work

### Implementation Status

All 12 plan items have their **code implementation complete**. Build, vet, and unit tests all pass.

| # | Item | Code | Unit Tests | Integration Tests |
|---|------|------|-----------|-------------------|
| 1 | Cascade Delete | DONE | - | **MISSING** |
| 2 | Tags in Responses | DONE | - | **MISSING** |
| 3 | UpdateTable | DONE | DONE (handler) | **MISSING** |
| 4 | UpdateView | DONE | - | **MISSING** |
| 5 | Column Comments | DONE | DONE (handler) | **MISSING** |
| 6 | Enhanced Search | DONE | - | **MISSING** |
| 7 | Lineage Deletion + TTL | DONE | DONE (service) | **MISSING** |
| 8 | Lineage Referential Integrity | DONE | - | - |
| 9 | Table Statistics | DONE | - | **MISSING** |
| 10 | Soft Delete (basic) | DONE | - | **MISSING** |
| 11 | Classifications | DONE | DONE (service) | **MISSING** |
| 12 | UpdateCatalog | DONE | DONE (handler) | **MISSING** |

### Existing Unit Tests (already written & passing)

- `internal/service/lineage_test.go` — `TestLineageService_DeleteEdge` (3 subtests), `TestLineageService_PurgeOlderThan` (3 subtests)
- `internal/service/tags_test.go` — `TestValidateClassificationTag` (14 subtests), `TestTagService_CreateTag_ClassificationValidation` (2 subtests)
- `internal/api/handler_test.go` — `TestAPI_UpdateTable` (3), `TestAPI_UpdateCatalog` (1), `TestAPI_UpdateColumn` (2), `TestAPI_UpdateEndpoints_Authorization` (5)

### Remaining Work: Integration HTTP Tests

All integration tests go in `test/integration/`. They use `//go:build integration` and `setupHTTPServer(t, httpTestOpts{...})`.

---

#### Test 1: `TestHTTP_UpdateTable` — `catalog_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Sequential steps:
1. **patch_comment_200** — `PATCH /v1/catalog/schemas/main/tables/titanic` with `{"comment": "Titanic dataset"}` → 200. Assert response has `comment` = "Titanic dataset".
2. **patch_properties_200** — PATCH same table with `{"properties": {"source": "kaggle"}}` → 200. Assert response has `properties.source` = "kaggle" AND `comment` still = "Titanic dataset" (partial update preserved).
3. **patch_owner_200** — PATCH with `{"owner": "data_team"}` → 200. Assert `owner` = "data_team".
4. **get_verifies_update** — `GET /v1/catalog/schemas/main/tables/titanic` → 200. Assert comment, properties, owner all persisted.
5. **patch_nonexistent_404** — PATCH `/v1/catalog/schemas/main/tables/nonexistent` → 404.

Auth: Use `env.Keys.Admin` for all requests.

#### Test 2: `TestHTTP_UpdateColumn` — `catalog_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Sequential steps:
1. **patch_comment_200** — `PATCH /v1/catalog/schemas/main/tables/titanic/columns/Name` with `{"comment": "Passenger name"}` → 200. Assert response `comment` = "Passenger name".
2. **get_table_verifies** — `GET /v1/catalog/schemas/main/tables/titanic` → 200. Find column "Name" in `columns` array, assert `comment` = "Passenger name".
3. **patch_nonexistent_column_404** — PATCH `.../columns/DoesNotExist` → 404.

Auth: `env.Keys.Admin`.

#### Test 3: `TestHTTP_UpdateCatalog` — `catalog_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Sequential steps:
1. **patch_comment_200** — `PATCH /v1/catalog` with `{"comment": "Production data lake"}` → 200. Assert `comment` = "Production data lake".
2. **get_verifies** — `GET /v1/catalog` → 200. Assert `comment` = "Production data lake".
3. **analyst_denied_403** — PATCH with `env.Keys.Analyst` → 403.

#### Test 4: `TestHTTP_ProfileTable` — `catalog_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Sequential steps:
1. **profile_200** — `POST /v1/catalog/schemas/main/tables/titanic/profile` (no body) → 200. Assert response has `column_count` = 12 (seeded column count), `last_profiled_at` is non-nil.
2. **get_table_has_stats** — `GET /v1/catalog/schemas/main/tables/titanic` → 200. Assert `statistics` field is present with `column_count` = 12.
3. **profile_nonexistent_404** — POST `.../tables/nonexistent/profile` → 404.

#### Test 5: `TestHTTP_TagsInSchemaResponse` — `catalog_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Sequential steps:
1. **create_tag** — `POST /v1/tags` with `{"key": "env", "value": "production"}` → 201. Capture `tag_id`.
2. **get_schema_id** — `GET /v1/catalog/schemas/main` → 200. Capture schema `schema_id` from response. (Note: the tag assignment needs the securable_id — the DuckLake schema_id. Need to check what format `securable_id` uses for schemas.)
3. **assign_tag** — `POST /v1/tags/{tag_id}/assignments` with `{"securable_type": "schema", "securable_id": <schema_id>}` → 201.
4. **get_schema_has_tags** — `GET /v1/catalog/schemas/main` → 200. Assert `tags` array contains an entry with `key` = "env" and `value` = "production".
5. **unassign_tag** — Cleanup: delete the assignment.

#### Test 6: `TestHTTP_TagsInTableResponse` — `catalog_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Same pattern as Test 5 but with `securable_type: "table"` and verifying `GET .../tables/titanic` includes tags.

#### Test 7: `TestHTTP_UpdateView` — `views_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Sequential steps:
1. **create_view** — `POST /v1/catalog/schemas/main/views` with `{"name": "v_test", "view_definition": "SELECT 1"}` → 201.
2. **patch_comment_200** — `PATCH /v1/catalog/schemas/main/views/v_test` with `{"comment": "Test view"}` → 200. Assert `comment` = "Test view".
3. **patch_definition_200** — PATCH with `{"view_definition": "SELECT 2"}` → 200. Assert `view_definition` = "SELECT 2" and `comment` still = "Test view".
4. **get_verifies** — `GET /v1/catalog/schemas/main/views/v_test` → 200. Assert comment and definition persisted.
5. **patch_nonexistent_404** — PATCH `.../views/nonexistent` → 404.
6. **analyst_denied_403** — PATCH with `env.Keys.Analyst` → 403.

#### Test 8: `TestHTTP_DeleteLineageEdge` — `lineage_http_test.go`

Setup: `httpTestOpts{}` (no DuckLake needed)

Steps:
1. **seed_edges** — Insert edges via `repository.NewLineageRepo(env.MetaDB)` + `repo.InsertEdge()` (same pattern as existing `seedLineageEdges`).
2. **delete_edge_204** — `DELETE /v1/lineage/edges/{id}` → 204.
3. **verify_gone** — `GET /v1/lineage?table_name=<source>` → 200. Assert the deleted edge is not in results.
4. **delete_nonexistent_404** — `DELETE /v1/lineage/edges/99999` → 404.

#### Test 9: `TestHTTP_PurgeLineage` — `lineage_http_test.go`

Setup: `httpTestOpts{}`

Steps:
1. **seed_old_edges** — Insert edges with `created_at` set to 100+ days ago via direct SQL.
2. **seed_recent_edge** — Insert a recent edge.
3. **purge_200** — `POST /v1/lineage/purge` with `{"older_than_days": 30}` → 200. Assert `deleted_count` >= 1.
4. **verify_recent_survives** — `GET /v1/lineage?table_name=<recent_source>` → 200. Assert recent edge still present.

#### Test 10: `TestHTTP_SearchByComment` — `search_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Steps:
1. **set_comment** — `PATCH /v1/catalog/schemas/main/tables/titanic` with `{"comment": "passenger survival data"}` → 200.
2. **search_by_comment** — `GET /v1/search?query=survival` → 200. Assert at least one result with `match_field` = "comment" and `name` = "titanic".

#### Test 11: `TestHTTP_SearchByTag` — `search_http_test.go`

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}`

Steps:
1. **create_tag** — `POST /v1/tags` with `{"key": "domain", "value": "maritime"}` → 201. Capture `tag_id`.
2. **assign_tag** — `POST /v1/tags/{tag_id}/assignments` with `{"securable_type": "table", "securable_id": 1}` → 201.
3. **search_by_tag** — `GET /v1/search?query=maritime` → 200. Assert at least one result with `match_field` = "tag".

#### Test 12: `TestHTTP_ListClassifications` — `tags_http_test.go` (or new file)

Setup: `httpTestOpts{SeedDuckLakeMetadata: true}` (migrations seed classification tags)

Steps:
1. **list_200** — `GET /v1/classifications` → 200. Assert `data` array contains seeded classification/sensitivity tags.
2. **verify_count** — Assert at least 8 tags (the seeded classifications).
3. **any_user_can_list** — `GET /v1/classifications` with `env.Keys.Analyst` → 200.

#### Test 13: `TestHTTP_CascadeDeleteVerifiesGovernanceRecords` — `catalog_http_test.go`

Setup: `httpTestOpts{WithDuckLake: true}` (needs real DuckLake for schema/table creation)

Steps:
1. **create_schema** — `POST /v1/catalog/schemas` → 201.
2. **create_table** — `POST /v1/catalog/schemas/{name}/tables` → 201.
3. **create_tag_and_assign** — Create tag, assign to table.
4. **create_row_filter** — `POST /v1/row-filters` for the table.
5. **create_column_mask** — `POST /v1/column-masks` for a column.
6. **force_delete_schema** — `DELETE /v1/catalog/schemas/{name}?force=true` → 204.
7. **verify_governance_cleaned** — Query `env.MetaDB` directly: assert 0 rows in `row_filters`, `column_masks`, `tag_assignments` for the deleted table/schema.

### Verification

After writing all integration tests:
1. `go test -race ./test/integration/... -count=1` — all new tests pass
2. `go build ./... && go vet ./... && go test -race ./...` — full suite green

---

## 1. Cascade Delete on Table Drop

**Problem:** Deleting a table leaves orphaned `row_filters`, `column_masks`, and `tag_assignments`.

**Files to modify:**
- `internal/service/catalog.go` — `DeleteTable` method
- `internal/db/repository/catalog.go` — `DeleteTable` method

**Changes:**
- In `CatalogRepo.DeleteTable`, after dropping the table and cleaning `catalog_metadata`, also delete:
  - `DELETE FROM row_filters WHERE table_id = ?`
  - `DELETE FROM column_masks WHERE table_id = ?` 
  - `DELETE FROM tag_assignments WHERE securable_type = 'table' AND securable_id = ?`
  - `DELETE FROM tag_assignments WHERE securable_type = 'column' AND securable_id = ?` (for column-level tags on this table)
- Need to resolve the DuckLake `table_id` before dropping the table (move the GetTable call earlier to capture it)
- Similarly for `DeleteSchema` with force: clean up all tables' governance data in the schema

**Tests:**
- Add test in `internal/db/repository/` or `internal/engine/` that creates a table with row filters + column masks + tags, then deletes it, and verifies all governance records are removed

---

## 2. Tags Included in GET Schema/Table Responses

**Problem:** Tags are a separate API — GET schema/table never returns associated tags.

**Files to modify:**
- `internal/domain/catalog.go` — Add `Tags []Tag` field to `SchemaDetail` and `TableDetail`
- `internal/service/catalog.go` — Inject `TagRepository` dependency; enrich responses with tags
- `internal/db/repository/catalog.go` — Add tag enrichment helper (or do it at service layer)
- `internal/api/handler.go` — Map tags to API response
- `internal/api/openapi.yaml` — Add `tags` array field to `SchemaDetail` and `TableDetail` response schemas

**Changes:**
- Add `TagRepository` to `CatalogService` constructor
- After getting schema/table from repo, call `tagRepo.ListTagsForSecurable("schema", schemaID, nil)` and attach
- For tables, also gather column-level tags
- Map tags to API response format in handler

**Tests:**
- Test that creating a tag, assigning it to a schema/table, then GET returns it inline

---

## 3. UpdateTable Endpoint

**Problem:** Tables cannot be updated after creation (no comment/properties/owner changes).

**Files to modify:**
- `internal/domain/catalog.go` — Add `UpdateTableRequest` type
- `internal/domain/repository.go` — Add `UpdateTable` to `CatalogRepository` interface
- `internal/db/repository/catalog.go` — Implement `UpdateTable`
- `internal/service/catalog.go` — Add `UpdateTable` with auth check
- `internal/api/openapi.yaml` — Add `PATCH /catalog/schemas/{schemaName}/tables/{tableName}`
- `internal/api/handler.go` — Add handler for `UpdateTableMetadata`

**Domain type:**
```go
type UpdateTableRequest struct {
    Comment    *string
    Properties map[string]string
    Owner      *string
}
```

**Repository implementation:**
- Same pattern as `UpdateSchema` — upsert into `catalog_metadata` with `ON CONFLICT DO UPDATE`
- Securable name = `schemaName.tableName`

**API spec:**
```yaml
patch:
  operationId: updateTableMetadata
  parameters:
    - name: schemaName
    - name: tableName
  requestBody:
    content:
      application/json:
        schema:
          $ref: '#/components/schemas/UpdateTableRequest'
  responses:
    200:
      $ref: '#/components/schemas/TableDetail'
```

**Tests:**
- Create table, update comment, verify. Update properties, verify. Test 404 for nonexistent table.

---

## 4. UpdateView Endpoint

**Problem:** Views can only be created and deleted, not updated.

**Files to modify:**
- `internal/domain/view.go` — Add `UpdateViewRequest` type
- `internal/domain/repository.go` — Add `Update` to `ViewRepository` interface
- `internal/db/queries/views.sql` — Add UPDATE query
- `internal/db/repository/view.go` — Implement `Update`
- `internal/service/view.go` — Add `UpdateView` with auth check
- `internal/api/openapi.yaml` — Add `PATCH /catalog/schemas/{schemaName}/views/{viewName}`
- `internal/api/handler.go` — Add handler

**Domain type:**
```go
type UpdateViewRequest struct {
    Comment        *string
    Properties     map[string]string
    ViewDefinition *string
}
```

**Repository:** Update `views` table SET comment/properties/view_definition WHERE schema_id AND name, with COALESCE for partial updates.

**Tests:**
- Create view, update comment, verify. Update view definition, verify source_tables re-extracted.

---

## 5. Column Comments Storage

**Problem:** `ColumnDetail.Comment` exists in domain but is never persisted or populated.

**Files to modify:**
- `internal/db/migrations/` — New migration `014_column_metadata.sql`
- `internal/domain/catalog.go` — Add `UpdateColumnRequest` type
- `internal/domain/repository.go` — Add `UpdateColumn` and `GetColumnMetadata` to `CatalogRepository`
- `internal/db/repository/catalog.go` — Implement column metadata storage + enrich `loadColumns`
- `internal/service/catalog.go` — Add `UpdateColumn` method
- `internal/api/openapi.yaml` — Add `PATCH /catalog/schemas/{schemaName}/tables/{tableName}/columns/{columnName}`
- `internal/api/handler.go` — Add handler

**Migration (014):**
```sql
CREATE TABLE column_metadata (
    table_securable_name TEXT NOT NULL, -- "schema.table"
    column_name          TEXT NOT NULL,
    comment              TEXT,
    properties           TEXT, -- JSON
    updated_at           TEXT DEFAULT (datetime('now')),
    UNIQUE(table_securable_name, column_name)
);
```

**Repository changes:**
- `loadColumns` and `ListColumns`: LEFT JOIN with `column_metadata` to populate `Comment`
- `UpdateColumn`: upsert into `column_metadata`

**Domain type:**
```go
type UpdateColumnRequest struct {
    Comment    *string
    Properties map[string]string
}
```

Add `Properties map[string]string` to `ColumnDetail` as well.

**Tests:**
- Create table, update column comment, GET table and verify column comment is populated
- Update column properties, verify

---

## 6. Enhanced Search (comments, properties, tags)

**Problem:** Search only matches entity names via LIKE. Doesn't search comments, properties, or tags.

**Files to modify:**
- `internal/db/repository/search.go` — Rewrite search SQL
- `internal/domain/search.go` — Ensure `MatchField` is properly returned

**Changes to search SQL:**
Extend each UNION branch to also match against `catalog_metadata.comment`, `catalog_metadata.properties`, and joined `tags`:

```sql
-- Schema search (name)
SELECT 'schema' AS type, s.schema_name AS name, ... , 'name' AS match_field
FROM ducklake_schema s
LEFT JOIN catalog_metadata cm ON cm.securable_type = 'schema' AND cm.securable_name = s.schema_name
WHERE s.end_snapshot IS NULL AND s.schema_name LIKE ?

UNION ALL

-- Schema search (comment)
SELECT 'schema' AS type, s.schema_name AS name, ... , 'comment' AS match_field
FROM ducklake_schema s
JOIN catalog_metadata cm ON cm.securable_type = 'schema' AND cm.securable_name = s.schema_name
WHERE s.end_snapshot IS NULL AND cm.comment LIKE ?

UNION ALL

-- Schema search (tag)
SELECT 'schema' AS type, s.schema_name AS name, ... , 'tag' AS match_field
FROM ducklake_schema s
JOIN tag_assignments ta ON ta.securable_type = 'schema' AND ta.securable_id = s.schema_id
JOIN tags t ON t.id = ta.tag_id
WHERE s.end_snapshot IS NULL AND (t.key LIKE ? OR t.value LIKE ?)

-- ... same pattern for tables and columns
```

- Add `end_snapshot IS NULL` filter (currently missing in search — could return deleted items)
- Deduplicate results (same entity could match in multiple fields — prefer name > comment > property > tag)

**Tests:**
- Create table with comment, search by comment substring, verify match_field = "comment"
- Create tag, assign to schema, search by tag key, verify match_field = "tag"

---

## 7. Lineage Deletion + TTL

**Problem:** Lineage edges can only be inserted, never deleted. Table grows unbounded.

**Files to modify:**
- `internal/domain/repository.go` — Add `DeleteEdge`, `DeleteEdgesOlderThan` to `LineageRepository`
- `internal/db/queries/lineage.sql` — Add DELETE queries
- `internal/db/repository/lineage.go` — Implement delete methods
- `internal/service/lineage.go` — Add `DeleteEdge`, `PurgeOldEdges`
- `internal/api/openapi.yaml` — Add `DELETE /lineage/edges/{edgeId}` and `POST /lineage/purge`
- `internal/api/handler.go` — Add handlers

**Changes:**
```sql
-- Delete specific edge
DELETE FROM lineage_edges WHERE id = ?;

-- TTL purge
DELETE FROM lineage_edges WHERE created_at < ?;
```

**API:**
- `DELETE /lineage/edges/{edgeId}` — delete a specific edge
- `POST /lineage/purge` — body: `{"older_than_days": 90}` — admin-only bulk purge

**Tests:**
- Insert edge, delete by ID, verify gone
- Insert old edges, purge with TTL, verify only old ones removed

---

## 8. Lineage Referential Integrity

**Problem:** `lineage_edges` uses free-form `source_table TEXT` / `target_table TEXT` — stale on rename/delete.

**Files to modify:**
- `internal/db/migrations/` — New migration `015_lineage_qualified_names.sql`
- `internal/domain/lineage.go` — Update `LineageEdge` fields
- `internal/db/repository/lineage.go` — Update queries
- `internal/service/query.go` — Update `emitLineage` to use qualified `schema.table` names
- `internal/api/handler.go` — Update lineage response mapping

**Migration (015):**
```sql
-- Add schema-qualified columns
ALTER TABLE lineage_edges ADD COLUMN source_schema TEXT;
ALTER TABLE lineage_edges ADD COLUMN target_schema TEXT;
-- Backfill: extract schema from existing names if they contain a dot, else default to 'main'
UPDATE lineage_edges SET source_schema = 'main' WHERE source_schema IS NULL;
UPDATE lineage_edges SET target_schema = 'main' WHERE target_schema IS NULL;
```

**Domain changes:**
- Add `SourceSchema`, `TargetSchema` to `LineageEdge`
- Store fully qualified names (`schema.table`) in `source_table`/`target_table` for backward compatibility
- On table delete (item #1), also delete lineage edges referencing that table

**Tests:**
- Insert lineage with qualified names, query upstream/downstream, verify schema fields populated

---

## 9. Table Statistics

**Problem:** No row count, size, or column count tracked per table.

**Files to modify:**
- `internal/db/migrations/` — New migration `016_table_statistics.sql`
- `internal/domain/catalog.go` — Add `TableStatistics` type, add to `TableDetail`
- `internal/domain/repository.go` — Add `TableStatisticsRepository` interface
- `internal/db/repository/table_statistics.go` — New repo implementation
- `internal/service/catalog.go` — Add `ProfileTable` method
- `internal/api/openapi.yaml` — Add `POST /catalog/schemas/{schemaName}/tables/{tableName}/profile` + add stats to GET table response
- `internal/api/handler.go` — Add handler

**Migration (016):**
```sql
CREATE TABLE table_statistics (
    table_securable_name TEXT NOT NULL UNIQUE, -- "schema.table"
    row_count            INTEGER,
    size_bytes           INTEGER,
    column_count         INTEGER,
    last_profiled_at     TEXT DEFAULT (datetime('now')),
    profiled_by          TEXT
);
```

**Domain type:**
```go
type TableStatistics struct {
    RowCount       *int64
    SizeBytes      *int64
    ColumnCount    *int64
    LastProfiledAt *time.Time
    ProfiledBy     string
}
```

**Profile endpoint:**
- `POST .../profile` runs DuckDB queries:
  - `SELECT COUNT(*) FROM lake."schema"."table"` → row count
  - Column count from `ducklake_column` count
  - Size from DuckDB system functions or DuckLake metadata if available
- Stores result in `table_statistics` table
- Returns the statistics

**GET table response:** Include `statistics` field (nullable — only present if profiled).

**Tests:**
- Create table, insert data, profile, verify row_count > 0
- GET table, verify statistics field present

---

## 10. Soft Delete for Catalog Objects

**Problem:** All deletes are hard deletes, losing all associated metadata.

**Files to modify:**
- `internal/db/migrations/` — New migration `017_soft_delete.sql`
- `internal/domain/catalog.go` — Add `DeletedAt *time.Time` to `SchemaDetail`, `TableDetail`; add `IncludeDeleted` to list params
- `internal/domain/view.go` — Add `DeletedAt *time.Time` to `ViewDetail`
- `internal/db/repository/catalog.go` — Change delete to set `deleted_at`, filter out deleted by default
- `internal/db/repository/view.go` — Same pattern
- `internal/service/catalog.go` — Add `RestoreSchema`, `RestoreTable`
- `internal/api/openapi.yaml` — Add `?include_deleted=true` query param to list endpoints; add `POST .../restore` endpoints
- `internal/api/handler.go` — Add handlers

**Migration (017):**
```sql
ALTER TABLE catalog_metadata ADD COLUMN deleted_at TEXT;
ALTER TABLE views ADD COLUMN deleted_at TEXT;
```

**Approach:**
- On delete: instead of removing from `catalog_metadata`, set `deleted_at = datetime('now')`. Still execute DuckDB `DROP TABLE/SCHEMA` for the actual data (DuckLake handles its own versioning via snapshots).
- List/Get queries add `AND (cm.deleted_at IS NULL)` by default
- With `?include_deleted=true`, show all including soft-deleted
- Restore endpoint: clear `deleted_at` + re-create the DuckDB object if needed (for schemas/views). For tables, restore is metadata-only (data may be gone from DuckLake).
- Hard delete: `DELETE /...?permanent=true` does actual removal

**Tests:**
- Soft delete a schema, verify it's hidden from list, visible with include_deleted
- Restore, verify it's back in normal list
- Hard delete with permanent=true, verify fully gone

---

## 11. Classifications via Extended Tags

**Problem:** No governed classification taxonomy for sensitivity labels, PII, etc.

**Files to modify:**
- `internal/domain/tag.go` — Add classification constants and validation
- `internal/service/tag.go` — Add validation for reserved prefixes; add seed classifications
- `internal/db/migrations/` — New migration `018_seed_classifications.sql`
- `internal/api/openapi.yaml` — Add `GET /classifications` convenience endpoint
- `internal/api/handler.go` — Add handler

**Reserved tag prefixes:**
```go
// Classification tag keys use the "classification:" prefix
const (
    ClassificationPrefix = "classification:"
    ClassPII            = "classification:pii"
    ClassSensitive      = "classification:sensitive"
    ClassConfidential   = "classification:confidential"
    ClassPublic         = "classification:public"
    ClassPersonalData   = "classification:personal_data"
)

// Sensitivity tag keys use "sensitivity:" prefix  
const (
    SensitivityPrefix   = "sensitivity:"
    SensHigh           = "sensitivity:high"
    SensMedium         = "sensitivity:medium"
    SensLow            = "sensitivity:low"
)
```

**Migration (018):** Seed these tags into the `tags` table:
```sql
INSERT OR IGNORE INTO tags (key, value, created_by, created_at) VALUES
('classification', 'pii', 'system', datetime('now')),
('classification', 'sensitive', 'system', datetime('now')),
('classification', 'confidential', 'system', datetime('now')),
('classification', 'public', 'system', datetime('now')),
('classification', 'personal_data', 'system', datetime('now')),
('sensitivity', 'high', 'system', datetime('now')),
('sensitivity', 'medium', 'system', datetime('now')),
('sensitivity', 'low', 'system', datetime('now'));
```

**Service validation:**
- `CreateTag`: allow custom tags, but validate `classification:*` and `sensitivity:*` keys only accept predefined values
- `GET /classifications`: convenience endpoint that returns `ListTags` filtered to reserved prefixes

**Tests:**
- Verify seed classifications exist after migration
- Assign classification tag to a column, verify it appears in GET table response
- Attempt to create `classification:invalid` → validation error

---

## 12. UpdateCatalog Comment Endpoint

**Problem:** Catalog comment is read-only — can be read from `GetCatalogInfo` but not set.

**Files to modify:**
- `internal/domain/repository.go` — Add `UpdateCatalog` to `CatalogRepository`
- `internal/db/repository/catalog.go` — Implement `UpdateCatalog`
- `internal/service/catalog.go` — Add `UpdateCatalog` with admin-only check
- `internal/api/openapi.yaml` — Add `PATCH /catalog`
- `internal/api/handler.go` — Add handler

**Changes:** Simple upsert into `catalog_metadata` with `securable_type='catalog'`, `securable_name='lake'`.

---

## File Change Summary

| File | Items |
|------|-------|
| `internal/domain/catalog.go` | #2, #3, #5, #9, #10, #12 |
| `internal/domain/repository.go` | #3, #4, #5, #7, #9, #12 |
| `internal/domain/view.go` | #4, #10 |
| `internal/domain/lineage.go` | #7, #8 |
| `internal/domain/tag.go` | #11 |
| `internal/service/catalog.go` | #1, #2, #3, #5, #9, #10, #12 |
| `internal/service/view.go` | #4 |
| `internal/service/lineage.go` | #7 |
| `internal/service/tag.go` | #11 |
| `internal/service/query.go` | #8 |
| `internal/db/repository/catalog.go` | #1, #3, #5, #9, #10, #12 |
| `internal/db/repository/view.go` | #4, #10 |
| `internal/db/repository/lineage.go` | #7, #8 |
| `internal/db/repository/search.go` | #6 |
| `internal/db/repository/table_statistics.go` | #9 (new) |
| `internal/db/queries/views.sql` | #4 |
| `internal/db/queries/lineage.sql` | #7 |
| `internal/api/openapi.yaml` | #2, #3, #4, #5, #6, #7, #9, #10, #11, #12 |
| `internal/api/handler.go` | #2, #3, #4, #5, #6, #7, #9, #10, #11, #12 |
| `cmd/server/main.go` | #2, #9 |
| Migrations (014-018) | #5, #8, #9, #10, #11 |

## New Files
- `internal/db/migrations/014_column_metadata.sql`
- `internal/db/migrations/015_lineage_qualified_names.sql`
- `internal/db/migrations/016_table_statistics.sql`
- `internal/db/migrations/017_soft_delete.sql`
- `internal/db/migrations/018_seed_classifications.sql`
- `internal/db/repository/table_statistics.go`

## Implementation Order

The items should be implemented in this order due to dependencies:

1. **#1 Cascade delete** — Foundational data integrity fix, no dependencies
2. **#2 Tags in responses** — Needed before #11 (classifications use tags)
3. **#3 UpdateTable** — Independent CRUD gap
4. **#4 UpdateView** — Independent CRUD gap
5. **#12 UpdateCatalog** — Small, independent
6. **#5 Column comments** — New migration, extends catalog_metadata pattern
7. **#6 Enhanced search** — Depends on column_metadata existing (#5)
8. **#7 Lineage deletion + TTL** — Independent
9. **#8 Lineage referential integrity** — Migration, independent of #7
10. **#9 Table statistics** — New table, new repo, independent
11. **#10 Soft delete** — Large cross-cutting change, do last among data items
12. **#11 Classifications** — Depends on #2 (tags in responses) and seed migration

## Verification

After each item:
1. `task build` — must pass
2. `task vet` — must pass
3. `task test` — must pass
4. New tests added for each feature

After all items:
1. `task build && task vet && task test` — all green
2. Manual verification: start server, create schema/table with comments, assign tags, search by comment, profile table, soft delete + restore
3. `task generate-api` after openapi.yaml changes — verify generated code matches
4. `task sqlc` after query changes — verify generated code matches

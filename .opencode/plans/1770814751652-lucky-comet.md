# Implementation Plan: DuckLake Data Platform Feature Expansion

## DuckLake Compatibility Notes

Key constraints discovered during analysis:

- **DuckLake only supports managed tables** -- no EXTERNAL table concept. All data files are Parquet, tracked in `ducklake_data_file`. Existing S3 Parquet files can be "adopted" via `ducklake_add_data_files()` but become DuckLake-managed after that.
- **DuckLake only supports Parquet** -- no CSV, JSON, Delta, or other formats.
- **DuckLake supports multiple catalogs** -- multiple `ATTACH` statements work in the same DuckDB session, each with its own SQLite metastore.
- **DuckLake has no volume concept** -- volumes are a pure application-layer construct.
- **Presigned URL system** is compatible with all features; needs per-cloud presigner implementations for Azure/GCS.

---

## Overview

Four features, ordered by dependency chain (Feature 3 from original plan revised significantly):

1. **Expanded Privileges & Securable Types** (prerequisite for everything else)
2. **Multi-Cloud Credentials** (S3 + Azure + GCS)
3. **Table Model Enrichment** (expose DuckLake native metadata: `nullable`, `storage_location` paths, column stats -- NOT external tables)
4. **Volumes CRUD** (governed file access, application-layer)
5. **Multi-Catalog Support** (largest change, last)

---

## Feature 1: Expanded Privileges & Securable Types

**Goal:** Extend the grant system to cover external locations, storage credentials, and volumes so RBAC applies uniformly instead of ad-hoc admin checks.

**DuckLake impact:** None. Pure application-layer SQLite metadata.

### Files to modify:

- `internal/domain/grant.go` -- Add constants:
  ```
  SecurableExternalLocation = "external_location"
  SecurableStorageCredential = "storage_credential"
  SecurableVolume = "volume"

  PrivCreateExternalLocation = "CREATE_EXTERNAL_LOCATION"
  PrivCreateStorageCredential = "CREATE_STORAGE_CREDENTIAL"
  PrivCreateVolume = "CREATE_VOLUME"
  PrivReadVolume = "READ_VOLUME"
  PrivWriteVolume = "WRITE_VOLUME"
  PrivReadFiles = "READ_FILES"
  PrivWriteFiles = "WRITE_FILES"
  ```

- `internal/service/authorization.go` -- Extend `checkPrivilegeForIdentities` switch (line 159) with new cases for `SecurableExternalLocation`, `SecurableStorageCredential`, `SecurableVolume`. Each follows: check direct grant -> inherit from catalog.

- `internal/service/storage_credential.go` -- Replace `requireCatalogAdmin()` calls (lines 122-131) with `CheckPrivilege(ctx, principal, SecurableCatalog, CatalogID, PrivCreateStorageCredential)`.

- `internal/service/external_location.go` -- Same: replace admin-only checks with `CheckPrivilege` using `PrivCreateExternalLocation`.

- `internal/api/openapi.yaml` -- Update `Privilege` and `SecurableType` enum definitions.

- Run `task generate-api`

### Tests:
- Extend `internal/service/authorization_test.go` with test cases for new securable types
- Update storage credential and external location service tests to verify privilege checks

### Verification:
```bash
task test
task build
```

---

## Feature 2: Multi-Cloud Credentials

**Goal:** Support Azure Blob Storage and GCS credentials alongside S3, including presigned URL generation for each cloud.

**DuckLake impact:** DuckDB natively supports `CREATE SECRET` for all three clouds. Presigner service needs Azure SAS token and GCS signed URL implementations.

### Files to modify:

**Domain & Validation:**

- `internal/domain/external_location.go`:
  - Add `CredentialTypeAzure = "AZURE"`, `CredentialTypeGCS = "GCS"`
  - Add `StorageTypeAzure = "AZURE"`, `StorageTypeGCS = "GCS"`
  - Extend `StorageCredential` with per-cloud fields:
    ```go
    // Azure
    AzureAccountName, AzureAccountKey     string
    AzureConnectionString                 string
    AzureTenantID, AzureClientID          string
    AzureClientSecret                     string
    // GCS
    GCSKeyFilePath string
    ```
  - Update `ValidateCreateStorageCredential()` and `ValidateCreateExternalLocation()` for new types

**DDL & Engine:**

- `internal/ddl/builder.go`:
  - Add `CreateAzureSecret(name, connectionString, accountName string)` -- `CREATE SECRET "name" (TYPE AZURE, CONNECTION_STRING '...')`
  - Add `CreateGCSSecret(name, keyFilePath string)` -- `CREATE SECRET "name" (TYPE GCS, KEY_FILE_PATH '...')`

- `internal/engine/engine.go`:
  - Add `CreateAzureSecret()` and `CreateGCSSecret()` wrappers
  - Rename `DropS3Secret` -> `DropSecret` (DuckDB `DROP SECRET` is type-agnostic)

**Service:**

- `internal/service/external_location.go`:
  - Switch on `credential.CredentialType` when creating DuckDB secrets
  - `RestoreSecrets()`: same switch for startup restore

- `internal/service/presigner.go`:
  - Add `AzurePresigner` struct implementing `Presigner` interface -- generates Azure SAS URLs
  - Add `GCSPresigner` struct implementing `Presigner` interface -- generates GCS signed URLs
  - Factory function `NewPresignerFromCredential(cred StorageCredential)` dispatches by type

- `internal/service/manifest.go`:
  - `resolvePresigner()` already dispatches per-schema -- just needs to handle new credential types via the factory

**Database:**

- `internal/db/migrations/022_multi_cloud_credentials.sql`:
  ```sql
  ALTER TABLE storage_credentials ADD COLUMN azure_account_name TEXT;
  ALTER TABLE storage_credentials ADD COLUMN azure_account_key_encrypted TEXT;
  ALTER TABLE storage_credentials ADD COLUMN azure_connection_string_encrypted TEXT;
  ALTER TABLE storage_credentials ADD COLUMN azure_tenant_id TEXT;
  ALTER TABLE storage_credentials ADD COLUMN azure_client_id TEXT;
  ALTER TABLE storage_credentials ADD COLUMN azure_client_secret_encrypted TEXT;
  ALTER TABLE storage_credentials ADD COLUMN gcs_key_file_path TEXT;
  ```

- `internal/db/queries/storage_credentials.sql` -- Update INSERT/SELECT for new columns
- `internal/db/repository/storage_credential.go` -- Encrypt/decrypt Azure/GCS fields
- Run `task sqlc`

**API:**

- `internal/api/openapi.yaml` -- Update `StorageCredential` and `CreateStorageCredential` schemas with cloud-specific fields
- Run `task generate-api`

### Tests:
- DDL builder tests for Azure/GCS secret generation in `internal/ddl/builder_test.go`
- Repository tests for Azure/GCS credential CRUD with encryption
- Service tests for secret creation dispatch by credential type
- Presigner unit tests for each cloud

### Verification:
```bash
task test
task build
```

---

## Feature 3: Table Model Enrichment (Revised)

**Goal:** Expose DuckLake's native metadata more fully. NOT adding external table types (DuckLake doesn't support them).

**What changed from original plan:** Dropped `TableType = "EXTERNAL"`, `DataSourceFormat` enum, and `StorageLocation` on tables. DuckLake only has managed Parquet tables. Instead we expose what DuckLake actually tracks.

### Changes:

**Column metadata enrichment:**

- `internal/domain/catalog.go` -- Extend `ColumnDetail`:
  ```go
  Nullable       bool   // from ducklake_column.nulls_allowed
  TypePrecision  *int   // parsed from column_type string if DECIMAL
  TypeScale      *int   // parsed from column_type string if DECIMAL
  ```

- `internal/db/repository/catalog.go`:
  - Column queries (lines 537-544): Add `nulls_allowed` to SELECT from `ducklake_column`
  - Parse precision/scale from DuckDB type strings (e.g., `DECIMAL(18,2)`) in the mapper

**Table storage path exposure:**

- `internal/domain/catalog.go` -- Extend `TableDetail`:
  ```go
  StoragePath string  // resolved full path (data_path + schema path + table path)
  ```

- `internal/db/repository/catalog.go`:
  - When building `TableDetail`, resolve the effective storage path from `ducklake_table.path` / `ducklake_schema.path` / `ducklake_metadata.data_path` (same resolution logic as `manifest.go:resolveDataFiles`)

**API:**

- `internal/api/openapi.yaml`:
  - Add `nullable` to `ColumnDetailResponse`
  - Add `storage_path` to `TableDetailResponse`
- Run `task generate-api`

- `internal/api/handler.go` -- Update response mappers

### Tests:
- Repository test: create table, verify `nullable` populated from DuckLake
- API test: verify column `nullable` field in responses

### Verification:
```bash
task test
task build
```

---

## Feature 4: Volumes CRUD

**Goal:** Governed access to unstructured files in cloud storage. Application-layer construct stored in SQLite, with presigned URL access.

**DuckLake impact:** None. Volumes are not a DuckLake concept. Stored in the application SQLite database alongside grants, principals, etc. Access via presigned URLs (same system as manifests).

### New files:

- `internal/domain/volume.go`:
  ```go
  type Volume struct {
      ID              int64
      Name            string
      SchemaName      string
      CatalogName     string
      VolumeType      string    // "MANAGED" or "EXTERNAL"
      StorageLocation string    // S3/Azure/GCS URL
      Comment         string
      Owner           string
      CreatedAt       time.Time
      UpdatedAt       time.Time
  }

  type CreateVolumeRequest struct {
      Name            string
      SchemaName      string
      VolumeType      string
      StorageLocation string  // required for EXTERNAL, auto-generated for MANAGED
      Comment         string
  }

  type UpdateVolumeRequest struct {
      NewName *string
      Comment *string
  }
  ```

- `internal/domain/repository.go` -- Add interface:
  ```go
  type VolumeRepository interface {
      Create(ctx context.Context, vol Volume) (Volume, error)
      Get(ctx context.Context, schemaName, name string) (Volume, error)
      List(ctx context.Context, schemaName string, page PageRequest) ([]Volume, int, error)
      Update(ctx context.Context, schemaName, name string, req UpdateVolumeRequest) (Volume, error)
      Delete(ctx context.Context, schemaName, name string) error
  }
  ```

- `internal/db/migrations/023_create_volumes.sql`:
  ```sql
  CREATE TABLE volumes (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      name TEXT NOT NULL,
      schema_name TEXT NOT NULL,
      catalog_name TEXT NOT NULL DEFAULT 'lake',
      volume_type TEXT NOT NULL CHECK(volume_type IN ('MANAGED', 'EXTERNAL')),
      storage_location TEXT,
      comment TEXT,
      owner TEXT,
      created_at TEXT NOT NULL DEFAULT (datetime('now')),
      updated_at TEXT NOT NULL DEFAULT (datetime('now')),
      UNIQUE(catalog_name, schema_name, name)
  );
  ```

- `internal/db/queries/volumes.sql` -- sqlc CRUD queries
- Run `task sqlc`

- `internal/db/repository/volume.go` -- Implements `VolumeRepository`

- `internal/service/volume.go`:
  ```go
  type VolumeService struct {
      repo    domain.VolumeRepository
      auth    domain.AuthorizationService
      audit   domain.AuditRepository
      catalog domain.CatalogRepository  // verify parent schema exists
  }
  ```
  - `Create`: Check `CREATE_VOLUME` on schema -> validate -> create
    - MANAGED: auto-generate `storage_location` under schema's data path
    - EXTERNAL: require `storage_location`, verify it's under a registered external location
  - `Get/List`: Check `USAGE` on schema
  - `Update`: Check ownership or admin
  - `Delete`: Check ownership or admin
  - All ops: best-effort audit logging

- `internal/api/openapi.yaml` -- Add endpoints:
  ```
  GET    /v1/catalog/schemas/{schemaName}/volumes              (listVolumes)
  POST   /v1/catalog/schemas/{schemaName}/volumes              (createVolume)
  GET    /v1/catalog/schemas/{schemaName}/volumes/{volumeName}  (getVolume)
  PATCH  /v1/catalog/schemas/{schemaName}/volumes/{volumeName}  (updateVolume)
  DELETE /v1/catalog/schemas/{schemaName}/volumes/{volumeName}  (deleteVolume)
  ```
  Add schemas: `VolumeResponse`, `CreateVolumeApiRequest`, `UpdateVolumeApiRequest`, `VolumeListResponse`

- Run `task generate-api`

- `internal/api/handler.go` -- Implement 5 handler methods (use views CRUD as template)

- `cmd/server/main.go` -- Wire `VolumeRepository` and `VolumeService`

### Volume file access via presigned URLs:

Optionally add a volume content endpoint:
```
POST /v1/catalog/schemas/{schemaName}/volumes/{volumeName}/presign
Body: { "path": "subdir/file.parquet", "operation": "READ" }
Response: { "url": "https://...", "expires_at": "..." }
```
This reuses the existing presigner infrastructure. `READ` generates GET URLs, `WRITE` generates PUT URLs. Authorization checks `READ_VOLUME` or `WRITE_VOLUME` privilege.

### Tests:
- `internal/db/repository/volume_test.go` -- CRUD with real SQLite (`t.TempDir()`)
- `internal/service/volume_test.go` -- Privilege checks, validation, audit
- `internal/api/volume_test.go` -- HTTP round-trip tests

### Verification:
```bash
task sqlc
task generate-api
task build
task test
```

---

## Feature 5: Multi-Catalog Support

**Goal:** Support multiple catalogs, each backed by a separate DuckLake SQLite metastore file + S3 data path.

**DuckLake compatibility:** Confirmed. DuckDB supports multiple `ATTACH` in the same session. Each gets an independent `__ducklake_metadata_<name>` database. Each catalog has its own SQLite metastore and data path.

### Phase 5a: Domain & Config

- `internal/config/config.go`:
  - Add `DefaultCatalogName string` (env: `DEFAULT_CATALOG_NAME`, default: `"lake"`)
  - Existing `MetaDBPath` and `DataPath` become the default catalog's paths

- `internal/domain/catalog.go` -- Extend `CatalogInfo`:
  ```go
  type CatalogInfo struct {
      ID          int64
      Name        string
      Comment     string
      Owner       string
      Properties  map[string]string
      StorageRoot string   // S3 data path
      MetaDBPath  string   // SQLite metastore file path
      CreatedAt   time.Time
      UpdatedAt   time.Time
      CreatedBy   string
      UpdatedBy   string
  }
  ```
  Add `CreateCatalogRequest`, `UpdateCatalogRequest`.

- `internal/domain/grant.go` -- Replace `CatalogID int64 = 0` sentinel with per-catalog IDs from the `catalogs` table.

- `internal/domain/repository.go`:
  - All `CatalogRepository` methods gain `catalogName` parameter
  - Add `CatalogManagementRepository` interface for catalog CRUD

### Phase 5b: DDL & Engine

- `internal/ddl/builder.go` -- Parameterize catalog name in all functions:
  - `CreateSchema(catalogName, schemaName)` -> `CREATE SCHEMA "catalogName"."schemaName"`
  - `CreateTable(catalogName, schema, name, cols)` -> `CREATE TABLE "catalogName"."schema"."name" (...)`
  - `AttachDuckLake(catalogName, metaDBPath, dataPath)` -> `ATTACH ... AS "catalogName"`
  - Remove all hardcoded `lake.` prefixes

- `internal/engine/engine.go`:
  - `IsCatalogAttached(name string)` -- parameterized
  - `AttachDuckLake(name, metaDBPath, dataPath)` -- parameterized
  - Remove `USE lake`; all DDL uses fully-qualified three-part names
  - Add `DetachCatalog(name string)` for catalog deletion

### Phase 5c: Database & Repository

- `internal/db/migrations/024_create_catalogs.sql`:
  ```sql
  CREATE TABLE catalogs (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      name TEXT NOT NULL UNIQUE,
      comment TEXT,
      owner TEXT,
      properties TEXT,
      storage_root TEXT,
      meta_db_path TEXT NOT NULL,
      data_path TEXT NOT NULL,
      created_at TEXT NOT NULL DEFAULT (datetime('now')),
      updated_at TEXT NOT NULL DEFAULT (datetime('now')),
      created_by TEXT,
      updated_by TEXT
  );
  ```
  Migration seeds existing "lake" catalog from current config.

- `internal/db/repository/catalog.go`:
  - Constructor gets a catalog registry (name -> SQLite connection) instead of a single `metaDB`
  - All DuckLake queries scoped by catalog: `SELECT * FROM <catalog>.ducklake_schema` or use per-catalog SQLite connection
  - `catalog_metadata.securable_name` changes from `"schema.table"` to `"catalog.schema.table"`

- `internal/db/repository/catalog_management.go` (new):
  - `Create`: insert into `catalogs`, create SQLite file, `ATTACH` via DuckDB
  - `Delete`: `DETACH`, remove SQLite file
  - `Get/List/Update`: CRUD against `catalogs` table

### Phase 5d: Service & API

- `internal/service/catalog.go` -- All methods gain `catalogName` parameter; authorization uses per-catalog IDs

- `internal/service/authorization.go` -- Look up catalog ID from `catalogs` table by name instead of using sentinel

- `internal/service/catalog_management.go` (new) -- Catalog lifecycle with RBAC

- `internal/api/openapi.yaml` -- Add catalog CRUD endpoints and `{catalogName}` path param to all catalog-scoped endpoints:
  ```
  /v1/catalogs                                              (list, create)
  /v1/catalogs/{catalogName}                                (get, update, delete)
  /v1/catalogs/{catalogName}/schemas/...                    (existing schema/table/view/volume endpoints)
  ```
  Keep `/v1/catalog/...` as alias for default catalog (backward compat)

- `cmd/server/main.go` -- Load all catalogs at startup, attach each to DuckDB

### Phase 5e: Manifest & Presigner Updates

- `internal/service/manifest.go` -- `resolveDataFiles()` and `resolvePresigner()` gain catalog context. Each catalog may use different S3 credentials/buckets.

- `internal/service/ingestion.go` -- Catalog-aware ingestion (which DuckLake metastore to register files in)

### Phase 5f: Migration of Existing Data

Migration script:
1. Insert existing "lake" catalog into `catalogs` with current `MetaDBPath` and `DataPath`
2. Update `catalog_metadata.securable_name` to include catalog prefix where needed
3. Update `privilege_grants` catalog-level rows (currently `securable_id = 0`) to point to new catalog row ID

### Tests:
- Catalog management CRUD unit tests
- Integration: create second catalog, create schemas/tables in each, query across catalogs
- Backward compat: existing single-catalog behavior works unchanged
- Grants: privilege isolation between catalogs

### Verification:
```bash
task sqlc
task generate-api
task build
task test
```

---

## Implementation Order & Dependencies

```
Feature 1: Expanded Privileges ──────────────────────┐
                                                      │
Feature 2: Multi-Cloud Credentials ──┐                │
                                     │                │
Feature 3: Table Model Enrichment ───┤  (can parallel │
                                     │   with 1)      │
Feature 4: Volumes CRUD ────────────┘─────────────────┘
     (depends on 1 for CREATE_VOLUME etc.)

Feature 5: Multi-Catalog ───────────────────────────────
     (do last, touches everything)
```

## Estimated Effort

| Feature | Effort | New Files | Modified Files |
|---------|--------|-----------|----------------|
| 1. Expanded Privileges | 1-2 days | 0 | ~6 |
| 2. Multi-Cloud Credentials | 2-3 days | 1 migration | ~8 |
| 3. Table Model Enrichment | 1-2 days | 0 | ~5 |
| 4. Volumes CRUD | 3-4 days | ~6 new | ~4 |
| 5. Multi-Catalog | 5-8 days | ~4 new | ~15+ |
| **Total** | **~12-19 days** | | |

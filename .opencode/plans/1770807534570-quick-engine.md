# Plan: Migrate Handwritten SQL to sqlc + Harden DuckDB DDL

## Part 1: Migrate Application-Owned Queries to sqlc

### Scope

47 handwritten SQL statements across 9 files that query **application-owned** SQLite tables. Excludes `ducklake_*` table reads (no schema in migrations) and DuckDB DDL (different engine).

### New sqlc Query Files to Create

#### `internal/db/queries/catalog_metadata.sql` (NEW file — 13 queries)
Covers all CRUD on `catalog_metadata` table:
- `GetCatalogMetadata` — SELECT by securable_type + securable_name
- `GetCatalogMetadataForSchema` — SELECT WHERE securable_type='schema'
- `GetCatalogMetadataForTable` — SELECT WHERE securable_type='table'  
- `GetCatalogMetadataForCatalog` — SELECT WHERE securable_type='catalog' AND securable_name='lake'
- `UpsertCatalogMetadata` — INSERT ON CONFLICT DO UPDATE (general purpose)
- `UpsertCatalogMetadataForCatalog` — INSERT ON CONFLICT for catalog-level comment
- `InsertOrReplaceCatalogMetadata` — INSERT OR REPLACE (for create flows)
- `DeleteCatalogMetadataByTypeAndName` — DELETE by securable_type + securable_name
- `DeleteCatalogMetadataByTypeAndNamePattern` — DELETE WHERE securable_name LIKE ?

#### `internal/db/queries/column_metadata.sql` (NEW file — 3 queries)
- `UpsertColumnMetadata` — INSERT ON CONFLICT DO UPDATE
- `GetColumnMetadata` — SELECT by table_securable_name + column_name
- `DeleteColumnMetadataByTable` — DELETE by table_securable_name
- `DeleteColumnMetadataByTablePattern` — DELETE WHERE table_securable_name LIKE ?

#### `internal/db/queries/table_statistics.sql` (NEW file — 3 queries)
- `UpsertTableStatistics` — INSERT ON CONFLICT DO UPDATE
- `GetTableStatistics` — SELECT by table_securable_name
- `DeleteTableStatistics` — DELETE by table_securable_name
- `DeleteTableStatisticsByPattern` — DELETE WHERE table_securable_name LIKE ?

#### Add Pagination Variants to Existing Query Files

Each needs a `Count*` + paginated `List*` pair:

| File | New Queries |
|------|-------------|
| `principals.sql` | `CountPrincipals`, `ListPrincipalsPaginated` (LIMIT/OFFSET) |
| `groups.sql` | `CountGroups`, `ListGroupsPaginated`, `CountGroupMembers`, `ListGroupMembersPaginated` |
| `privileges.sql` | `CountGrantsForPrincipal`, `ListGrantsForPrincipalPaginated`, `CountGrantsForSecurable`, `ListGrantsForSecurablePaginated` |
| `column_masks.sql` | `CountColumnMasksForTable`, `ListColumnMasksForTablePaginated` |
| `row_filters.sql` | `CountRowFiltersForTable`, `ListRowFiltersForTablePaginated` |

#### Add Cascade Delete Queries to Existing Files

| File | New Queries |
|------|-------------|
| `row_filters.sql` | `DeleteRowFiltersByTable` |
| `column_masks.sql` | `DeleteColumnMasksByTable` |
| `tags.sql` | `DeleteTagAssignmentsBySecurable`, `DeleteTagAssignmentsBySecurableTypes` |
| `lineage.sql` | `DeleteLineageEdge`, `PurgeLineageOlderThan`, `DeleteLineageByTable`, `DeleteLineageByTablePattern` |
| `views.sql` | `UpdateView`, `DeleteViewsBySchema` |

### Migration Steps (in order)

1. **Create 3 new `.sql` query files** (`catalog_metadata.sql`, `column_metadata.sql`, `table_statistics.sql`)
2. **Add pagination + cascade queries** to existing `.sql` files
3. **Run `task sqlc`** to regenerate `internal/db/dbstore/`
4. **Update `table_statistics.go`** — replace all 3 handwritten queries with sqlc calls (smallest file, good smoke test)
5. **Update `views.go`** — replace `Update` method with sqlc `UpdateView`
6. **Update `lineage.go`** — replace `DeleteEdge` and `PurgeOlderThan` with sqlc calls
7. **Update `principal.go`** — replace `List` with sqlc `CountPrincipals` + `ListPrincipalsPaginated`
8. **Update `group.go`** — replace `List` and `ListMembers` with paginated sqlc variants
9. **Update `grant.go`** — replace `ListForPrincipal` and `ListForSecurable`
10. **Update `column_mask.go`** — replace `GetForTable`
11. **Update `row_filter.go`** — replace `GetForTable`
12. **Update `catalog.go`** — replace all `catalog_metadata`, `column_metadata`, and `table_statistics` queries with sqlc calls. This is the largest change (~17 queries + ~10 cascade deletes). The DuckDB DDL and `ducklake_*` reads remain handwritten.
13. **Run `task test`** — verify everything passes
14. **Remove unused old sqlc queries** — the non-paginated `ListPrincipals`, `ListGroups`, etc. can be removed if no longer called anywhere

### Notes
- Sorting: some existing sqlc queries sort by `name`, handwritten ones sort by `id`. Standardize on whatever the API contract expects (check tests).
- The `CatalogRepo` will still need `metaDB *sql.DB` for the `ducklake_*` reads, but the `catalog_metadata`/`column_metadata` queries will go through `dbstore.Queries`. The repo constructor will need to accept both a raw `*sql.DB` (for ducklake reads) and a `*dbstore.Queries` (for sqlc calls), or create the `Queries` wrapper internally.

---

## Part 2: Harden DuckDB DDL Construction

### Current State

There are **9 DDL statements** across 2 files that use `fmt.Sprintf` to interpolate values into SQL:

**`catalog.go`** (4 DDL via `r.duckDB`):
- `CREATE SCHEMA lake."<name>"` (line 107)
- `DROP SCHEMA lake."<name>" [CASCADE]` (line 233)
- `CREATE TABLE lake."<schema>"."<name>" (<col defs>)` (line 302)
- `DROP TABLE lake."<schema>"."<name>"` (line 432)

**`engine.go`** (5 DDL via DuckDB):
- `CREATE SECRET "<name>" (...)` (line 170) — manual `""`/`''` escaping
- `DROP SECRET IF EXISTS "<name>"` (line 193) — manual `""` escaping
- `ATTACH 'ducklake:sqlite:<path>' AS lake (DATA_PATH '<path>')` (line 202) — `metaDBPath` NOT escaped

### Identified Risks

1. **Column type injection** (`catalog.go:299`): `c.Type` (e.g. `VARCHAR`) is interpolated directly into DDL with zero validation. A malicious type like `INTEGER); DROP TABLE foo; --` goes straight in.
2. **`metaDBPath` not escaped** (`engine.go:202`): interpolated into a string literal without `'` → `''` escaping.
3. **Inconsistent escaping**: three different approaches (regex allowlist, manual ReplaceAll, `sqlrewrite.QuoteIdentifier`) with no shared utility.

### Recommended Approach: `internal/ddl` Package

Create a small, focused `internal/ddl` package (NOT a full query builder) that provides:

#### 1. `ValidateIdentifier(name string) error`
Move and export the existing `validateIdentifier` from `catalog.go`. Keep the strict allowlist (`^[a-zA-Z_][a-zA-Z0-9_]*$`, max 128 chars). This is the first line of defense.

#### 2. `QuoteIdentifier(name string) string`
Consolidate on the approach already in `sqlrewrite.QuoteIdentifier` (double-quote, escape `"` → `""`). Use DuckDB's documented quoting rules: https://duckdb.org/docs/stable/sql/dialect/keywords_and_identifiers.html

#### 3. `QuoteLiteral(value string) string`
New helper for string literal values (single-quote, escape `'` → `''`). Replaces the scattered `strings.ReplaceAll(val, "'", "''")` in `engine.go`.

#### 4. `ValidateColumnType(typeName string) error`
New function to validate column type strings. Two options:
- **Option A (recommended): Allowlist of known DuckDB types** — `INTEGER`, `VARCHAR`, `BOOLEAN`, `TIMESTAMP`, `DOUBLE`, `BIGINT`, `DATE`, `BLOB`, `FLOAT`, `SMALLINT`, `TINYINT`, `HUGEINT`, `DECIMAL(p,s)`, `VARCHAR(n)`, etc. Parse with a regex that accepts `WORD` or `WORD(digits)` or `WORD(digits,digits)`. Rejects anything else.
- **Option B: Parse-based** — attempt `pg_query.Parse("SELECT 1::" + typeName)` to verify it's a valid type expression. Downside: pg_query uses PostgreSQL grammar, not DuckDB grammar, so some valid DuckDB types might be rejected.

#### 5. DDL Builder Functions

```go
// Returns: CREATE SCHEMA lake."<name>"
func CreateSchema(name string) (string, error)

// Returns: DROP SCHEMA lake."<name>" [CASCADE]
func DropSchema(name string, cascade bool) (string, error)

// Returns: CREATE TABLE lake."<schema>"."<name>" ("<col1>" TYPE1, "<col2>" TYPE2)
func CreateTable(schema, table string, columns []ColumnDef) (string, error)

// Returns: DROP TABLE lake."<schema>"."<name>"
func DropTable(schema, table string) (string, error)

// Returns: CREATE SECRET "<name>" (TYPE S3, KEY_ID '...', ...)
func CreateS3Secret(name, keyID, secret, endpoint, region, urlStyle string) (string, error)

// Returns: DROP SECRET IF EXISTS "<name>"
func DropS3Secret(name string) (string, error)

// Returns: ATTACH 'ducklake:sqlite:<metaPath>' AS lake (DATA_PATH '<dataPath>')
func AttachDuckLake(metaDBPath, dataPath string) (string, error)
```

Each function:
- Calls `ValidateIdentifier` / `ValidateColumnType` as appropriate
- Uses `QuoteIdentifier` / `QuoteLiteral` for all interpolation
- Returns the SQL string + error (never panics)
- Is **easily testable** — pure function, no DB dependency

### Why Not a General Query Builder (Squirrel, goqu, etc.)?

- **Squirrel** doesn't do identifier quoting (open issue #94 since 2017), doesn't support DDL at all (only SELECT/INSERT/UPDATE/DELETE), and has known security audit warnings (GO-S1017).
- **goqu** is MySQL/PostgreSQL focused and doesn't support DuckDB-specific syntax (`CREATE SECRET`, `ATTACH`, `URL_STYLE`).
- **ddlite** is SQLite-only.
- **safequery** (`github.com/rudderlabs/safequery`) is interesting for its `$$1` identifier escaping pattern but is minimal, relatively unknown, and still wouldn't handle DuckDB-specific DDL.

The DDL surface is small (7 distinct statement shapes). A focused `internal/ddl` package with builder functions is simpler, safer, and more maintainable than adding a dependency.

### Migration Steps

1. **Create `internal/ddl/` package** with `identifier.go` (ValidateIdentifier, QuoteIdentifier, QuoteLiteral, ValidateColumnType) and `builder.go` (the 7 builder functions)
2. **Write tests** (`internal/ddl/identifier_test.go`, `internal/ddl/builder_test.go`) — table-driven tests covering valid inputs, injection attempts, edge cases (empty string, max length, special chars, SQL keywords in type names)
3. **Update `catalog.go`** — replace `fmt.Sprintf` DDL with `ddl.CreateSchema()`, etc. Remove local `validateIdentifier`.
4. **Update `engine.go`** — replace manual escaping with `ddl.CreateS3Secret()`, `ddl.DropS3Secret()`, `ddl.AttachDuckLake()`
5. **Fix `metaDBPath` escaping bug** in `AttachDuckLake` (currently not escaped at all)
6. **Run `task test`** — verify everything passes

---

## Verification

After both parts:
```bash
task sqlc          # regenerate dbstore
task build         # compile check
task test          # all unit tests pass
task vet           # static analysis clean
```

Review that:
- No handwritten SQL remains for application-owned tables (except the `ducklake_*` reads and `search.go` dynamic query)
- All DDL construction goes through `internal/ddl` builder functions
- No `fmt.Sprintf` with user input exists in DDL paths
- Column type validation blocks injection attempts

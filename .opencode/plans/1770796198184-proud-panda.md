# Duck Lake vs Databricks: Feature Gap Analysis & Implementation Plan

## Part 1: Gap Analysis (Reference)

<details>
<summary>What Duck Lake Already Has (click to expand)</summary>

| Feature | Duck Lake | Databricks |
|---|---|---|
| Three-level namespace | Single catalog, schemas, tables | Unity Catalog object model |
| RBAC with hierarchical grants | Full privilege model | Unity Catalog privileges |
| Row-level security | SQL expression filters with bindings | Dynamic views / row filters |
| Column masking | Expression-based with `see_original` | Column masks |
| Group nesting | Transitive via BFS | Account-level groups |
| Audit logging | All operations logged | System tables |
| JWT + API key auth | Dual auth | Token-based |
| Managed tables | DuckLake Parquet on S3 | Managed Delta tables |
| Schema/table CRUD | Full lifecycle with metadata | Unity Catalog CRUD |
| Client-side DuckDB extension | Replacement scan + manifest | N/A (unique) |

</details>

<details>
<summary>20 Missing Features Identified (click to expand)</summary>

**P1 Core**: Data Lineage, Views, Multi-Catalog, Search, Tags, Data Sharing  
**P2 Platform**: External Tables, Volumes, Storage Credentials, Federation, UDFs, ML Models  
**P3 Ops**: Data Quality, System Tables, Query History, Multi-Tenancy, ABAC, Info Schema, Clean Rooms, SCIM

</details>

---

## Part 2: Implementation Plan (6 Selected Features)

### Implementation Order

| # | Feature | Effort | New Files | Modified Files | Migration |
|---|---------|--------|-----------|----------------|-----------|
| 1 | Query History | Low | 2-3 | 3-4 | 010 |
| 2 | Search API | Low | 3-4 | 3-4 | None |
| 3 | Data Lineage | Medium | 5-6 | 3-4 | 011 |
| 4 | Tagging System | Medium | 5-6 | 3-4 | 012 |
| 5 | Views Support | High | 5-6 | 5-6 | 013 |
| 6 | Information Schema | Medium | 2-3 | 2-3 | None |

---

### Feature 1: Query History API

**Goal**: Dedicated query history endpoint with richer metadata than audit logs.

**Migration 010** (`internal/db/migrations/010_add_query_history_fields.sql`):
```sql
ALTER TABLE audit_log ADD COLUMN rows_returned INTEGER;
```

**Domain** (`internal/domain/query_history.go`):
```go
type QueryHistoryEntry struct {
    ID             int64
    PrincipalName  string
    OriginalSQL    string
    RewrittenSQL   *string
    StatementType  *string
    TablesAccessed *string
    Status         string
    ErrorMessage   *string
    DurationMs     *int64
    RowsReturned   *int64
    CreatedAt      time.Time
}

type QueryHistoryRepository interface {
    List(ctx context.Context, filters QueryHistoryFilters) ([]QueryHistoryEntry, string, error)
}

type QueryHistoryFilters struct {
    PrincipalName *string
    Status        *string
    From          *time.Time
    To            *time.Time
    MaxResults    int
    PageToken     string
}
```

**Files to create**:
- `internal/domain/query_history.go` — types
- `internal/db/queries/query_history.sql` — SQL queries -> run `task sqlc`
- `internal/db/repository/query_history.go` — repository impl

**Files to modify**:
- `internal/domain/repository.go` — add `QueryHistoryRepository` interface
- `internal/service/query.go` — capture `rows_returned` in audit entry
- `internal/api/openapi.yaml` — add `GET /v1/query-history` endpoint
- `internal/api/handler.go` — implement handler (then `task generate-api`)
- `cmd/server/main.go` — wire up

**API**: `GET /v1/query-history?principal_name=&status=&from=&to=&max_results=&page_token=`

**Tests**: Table-driven tests in `internal/service/query_test.go` (or new file). Verify rows_returned is captured. Verify date range filtering.

---

### Feature 2: Search API

**Goal**: Full-text search across schemas, tables, columns by name/comment/properties.

**No migration needed** — queries existing DuckLake + catalog_metadata tables.

**Domain** (`internal/domain/search.go`):
```go
type SearchResult struct {
    Type       string  // "schema", "table", "column"
    Name       string
    SchemaName *string
    TableName  *string
    Comment    *string
    MatchField string  // which field matched: "name", "comment", "property"
}

type SearchRepository interface {
    Search(ctx context.Context, query string, objectType *string, maxResults int, offset int) ([]SearchResult, error)
}
```

**Implementation**: Raw SQL queries (not sqlc) because they JOIN across DuckLake tables (`ducklake_schema`, `ducklake_table`, `ducklake_column`) and `catalog_metadata`. Use `LIKE '%' || ? || '%'` for matching.

**Files to create**:
- `internal/domain/search.go` — types
- `internal/db/repository/search.go` — raw SQL repository (not sqlc-generated)
- `internal/service/search.go` — service with auth checks

**Files to modify**:
- `internal/domain/repository.go` — add `SearchRepository` interface
- `internal/api/openapi.yaml` — add `GET /v1/search`
- `internal/api/handler.go` — implement handler
- `cmd/server/main.go` — wire up

**API**: `GET /v1/search?query=<term>&type=schema|table|column&max_results=&page_token=`

**Tests**: Search by name, by comment, by property value. Verify type filtering. Verify empty results.

---

### Feature 3: Data Lineage

**Goal**: Track table-level lineage from query execution. Build a queryable lineage graph.

**Migration 011** (`internal/db/migrations/011_create_lineage_edges.sql`):
```sql
CREATE TABLE IF NOT EXISTS lineage_edges (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    source_table    TEXT NOT NULL,
    target_table    TEXT,              -- NULL for pure SELECT (read-only)
    edge_type       TEXT NOT NULL,     -- 'READ', 'WRITE', 'READ_WRITE'
    principal_name  TEXT NOT NULL,
    query_hash      TEXT,              -- SHA-256 of original SQL (for dedup)
    created_at      DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX idx_lineage_source ON lineage_edges(source_table);
CREATE INDEX idx_lineage_target ON lineage_edges(target_table);
CREATE INDEX idx_lineage_created ON lineage_edges(created_at);
```

**Domain** (`internal/domain/lineage.go`):
```go
type LineageEdge struct {
    ID            int64
    SourceTable   string
    TargetTable   *string
    EdgeType      string   // "READ", "WRITE", "READ_WRITE"
    PrincipalName string
    CreatedAt     time.Time
}

type LineageNode struct {
    TableName string
    Upstream  []LineageEdge  // tables that feed into this table
    Downstream []LineageEdge // tables that consume from this table
}

type LineageRepository interface {
    InsertEdge(ctx context.Context, edge LineageEdge) error
    GetUpstream(ctx context.Context, tableName string, depth int) ([]LineageEdge, error)
    GetDownstream(ctx context.Context, tableName string, depth int) ([]LineageEdge, error)
}
```

**Key logic** — after query execution in `service/query.go`:
- For `SELECT`: all extracted tables are sources, edge_type = "READ", target = nil
- For `INSERT INTO ... SELECT`: target = DML target table, sources = tables in SELECT subquery, edge_type = "WRITE" for target, "READ" for sources
- For `UPDATE/DELETE`: target = DML target table, edge_type = "WRITE"
- Lineage recording is best-effort (like audit logging): `_ = s.lineage.InsertEdge(...)`

**New sqlrewrite function** — `ExtractTargetTable(sql string) (*string, error)` to extract the target table from INSERT/UPDATE/DELETE statements.

**Files to create**:
- `internal/domain/lineage.go`
- `internal/db/queries/lineage.sql` -> `task sqlc`
- `internal/db/repository/lineage.go`
- `internal/service/lineage.go`

**Files to modify**:
- `internal/domain/repository.go` — add `LineageRepository`
- `internal/sqlrewrite/sqlrewrite.go` — add `ExtractTargetTable()`
- `internal/service/query.go` — emit lineage edges after execution
- `internal/api/openapi.yaml` — add lineage endpoints
- `internal/api/handler.go` — implement handlers
- `cmd/server/main.go` — wire up

**API**:
- `GET /v1/lineage/tables/{schemaName}/{tableName}` — full lineage (upstream + downstream)
- `GET /v1/lineage/tables/{schemaName}/{tableName}/upstream` — what feeds into this table
- `GET /v1/lineage/tables/{schemaName}/{tableName}/downstream` — what consumes from this table

**Tests**: 
- Unit test `ExtractTargetTable` for INSERT, UPDATE, DELETE, SELECT
- Service test: execute INSERT INTO...SELECT, verify lineage edges created
- API test: query lineage endpoints, verify upstream/downstream

---

### Feature 4: Tagging System

**Goal**: Formal tags on schemas, tables, columns with full CRUD API.

**Migration 012** (`internal/db/migrations/012_create_tags.sql`):
```sql
CREATE TABLE IF NOT EXISTS tags (
    id          INTEGER PRIMARY KEY AUTOINCREMENT,
    key         TEXT NOT NULL,
    value       TEXT,                  -- nullable (simple tags vs key:value)
    created_by  TEXT NOT NULL,
    created_at  DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(key, value)
);

CREATE TABLE IF NOT EXISTS tag_assignments (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    tag_id          INTEGER NOT NULL REFERENCES tags(id) ON DELETE CASCADE,
    securable_type  TEXT NOT NULL,      -- 'schema', 'table', 'column'
    securable_id    INTEGER NOT NULL,
    column_name     TEXT,               -- non-null only for column-level tags
    assigned_by     TEXT NOT NULL,
    assigned_at     DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(tag_id, securable_type, securable_id, column_name)
);
CREATE INDEX idx_tag_assignments_securable ON tag_assignments(securable_type, securable_id);
```

**Domain** (`internal/domain/tag.go`):
```go
type Tag struct {
    ID        int64
    Key       string
    Value     *string
    CreatedBy string
    CreatedAt time.Time
}

type TagAssignment struct {
    ID             int64
    TagID          int64
    SecurableType  string
    SecurableID    int64
    ColumnName     *string
    AssignedBy     string
    AssignedAt     time.Time
}

type TagRepository interface {
    CreateTag(ctx context.Context, tag Tag) (int64, error)
    GetTag(ctx context.Context, id int64) (Tag, error)
    ListTags(ctx context.Context, maxResults int, pageToken string) ([]Tag, string, error)
    DeleteTag(ctx context.Context, id int64) error
    AssignTag(ctx context.Context, assignment TagAssignment) (int64, error)
    UnassignTag(ctx context.Context, id int64) error
    ListTagsForSecurable(ctx context.Context, securableType string, securableID int64) ([]Tag, error)
    ListAssignmentsForTag(ctx context.Context, tagID int64) ([]TagAssignment, error)
}
```

**Files to create**:
- `internal/domain/tag.go`
- `internal/db/queries/tags.sql` -> `task sqlc`
- `internal/db/repository/tags.go`
- `internal/service/tags.go`

**Files to modify**:
- `internal/domain/repository.go` — add `TagRepository`
- `internal/api/openapi.yaml` — add tag endpoints
- `internal/api/handler.go` — implement handlers
- `cmd/server/main.go` — wire up
- `internal/service/search.go` — include tags in search results

**API**:
- `POST /v1/tags` — create tag
- `GET /v1/tags` — list tags (paginated)
- `DELETE /v1/tags/{id}` — delete tag (cascades)
- `POST /v1/tags/{id}/assignments` — assign to securable
- `DELETE /v1/tag-assignments/{id}` — unassign
- `GET /v1/catalog/schemas/{name}/tags` — tags for schema
- `GET /v1/catalog/schemas/{name}/tables/{name}/tags` — tags for table
- `GET /v1/catalog/schemas/{name}/tables/{name}/columns/{name}/tags` — tags for column

**Tests**: Create/list/delete tags. Assign/unassign. List by securable. Cascade delete. Duplicate tag conflict.

---

### Feature 5: Views Support

**Goal**: CREATE VIEW as a first-class catalog object with security enforcement.

**Migration 013** (`internal/db/migrations/013_create_views.sql`):
```sql
CREATE TABLE IF NOT EXISTS views (
    id               INTEGER PRIMARY KEY AUTOINCREMENT,
    schema_id        INTEGER NOT NULL,
    name             TEXT NOT NULL,
    view_definition  TEXT NOT NULL,      -- the SQL SELECT statement
    comment          TEXT,
    properties       TEXT DEFAULT '{}',  -- JSON key-value
    owner            TEXT NOT NULL,
    source_tables    TEXT,               -- JSON array of referenced table names
    created_at       DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at       DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(schema_id, name)
);
```

**Domain** (`internal/domain/view.go`):
```go
type ViewDetail struct {
    ID              int64
    SchemaName      string
    CatalogName     string
    Name            string
    ViewDefinition  string
    Comment         *string
    Properties      map[string]string
    Owner           string
    SourceTables    []string
    CreatedAt       time.Time
    UpdatedAt       time.Time
}

type ViewRepository interface {
    Create(ctx context.Context, view ViewDetail) (int64, error)
    GetByName(ctx context.Context, schemaName, viewName string) (ViewDetail, error)
    List(ctx context.Context, schemaName string, maxResults int, pageToken string) ([]ViewDetail, string, error)
    Delete(ctx context.Context, schemaName, viewName string) error
}
```

**Key design decisions**:
1. **Dual-write**: Create VIEW in DuckDB AND store metadata in SQLite
2. **DDL remains blocked** for raw SQL — views are only created via the API endpoint
3. **Privilege model**: Creating requires `CREATE_TABLE` on schema (or add new `CREATE_VIEW`). Querying requires `SELECT` on the view. At creation time, verify principal has SELECT on all source tables.
4. **RLS/masking**: When querying a view, row filters and column masks apply to the **underlying tables** (DuckDB handles this since the view expands to its definition). The sqlrewrite layer already handles subqueries.
5. **Lineage**: Record lineage edges from source tables to the view at creation time.

**Files to create**:
- `internal/domain/view.go`
- `internal/db/queries/views.sql` -> `task sqlc`
- `internal/db/repository/views.go`
- `internal/service/views.go`
- `internal/db/mapper/view.go` — if needed for DB<->domain mapping

**Files to modify**:
- `internal/domain/repository.go` — add `ViewRepository`
- `internal/engine/engine.go` — allow VIEW queries to pass through (or handle via service)
- `internal/api/openapi.yaml` — add view endpoints
- `internal/api/handler.go` — implement handlers
- `cmd/server/main.go` — wire up
- `internal/service/lineage.go` — record view lineage at creation

**API**:
- `POST /v1/catalog/schemas/{name}/views` — create view
- `GET /v1/catalog/schemas/{name}/views` — list views (paginated)
- `GET /v1/catalog/schemas/{name}/views/{name}` — get view
- `DELETE /v1/catalog/schemas/{name}/views/{name}` — drop view

**Tests**: Create view, query via /v1/query, verify RLS applies to underlying tables, verify privilege checks, verify lineage edges.

---

### Feature 6: Information Schema

**Goal**: Expose catalog metadata as SQL-queryable virtual tables.

**No migration needed** — this is a query-engine enhancement.

**Approach**: Intercept queries referencing `information_schema.*` in the `SecureEngine` and return results programmatically, filtered by the principal's privileges.

**Virtual tables to support**:
- `information_schema.schemata` — (catalog_name, schema_name, schema_owner, comment)
- `information_schema.tables` — (table_catalog, table_schema, table_name, table_type, comment)
- `information_schema.columns` — (table_catalog, table_schema, table_name, column_name, ordinal_position, data_type, comment)
- `information_schema.table_privileges` — (grantor, grantee, table_catalog, table_schema, table_name, privilege_type)

**Implementation**:
1. In `SecureEngine.Query()`, after SQL parsing, detect if the query references `information_schema.*`
2. If yes, build the result set programmatically:
   - Fetch all schemas/tables/columns via `CatalogRepository`
   - Filter by principal's privileges (only show objects the principal can access)
   - Construct a DuckDB `SELECT * FROM (VALUES ...)` query with the filtered metadata
   - Execute that against DuckDB
3. This avoids creating persistent views while still allowing SQL access

**Files to create**:
- `internal/engine/information_schema.go` — builds virtual result sets
- `internal/engine/information_schema_test.go`

**Files to modify**:
- `internal/engine/engine.go` — add information_schema detection and routing
- `internal/sqlrewrite/sqlrewrite.go` — add `ExtractSchemaReferences()` to detect information_schema usage

**Tests**: Query `SELECT * FROM information_schema.tables`, verify only authorized tables shown. Verify column metadata. Verify privilege filtering.

---

## Verification Plan

After implementing each feature:

1. **Unit tests**: `task test` — all existing + new tests pass
2. **Build**: `task build` — no compilation errors  
3. **Vet**: `task vet` — no static analysis issues
4. **Manual smoke test**: `go run ./cmd/server` and exercise new endpoints via curl
5. **Integration tests**: `task integration-test` (if extension is built) — existing tests still pass

### Per-feature verification:

| Feature | Test Command | Manual Verification |
|---|---|---|
| Query History | `go test ./internal/service/... -run TestQueryHistory` | `curl /v1/query-history` returns entries with rows_returned |
| Search | `go test ./internal/service/... -run TestSearch` | `curl '/v1/search?query=titanic'` finds the table |
| Lineage | `go test ./internal/service/... -run TestLineage` | Execute query, then `curl /v1/lineage/tables/main/titanic` |
| Tags | `go test ./internal/service/... -run TestTag` | Create tag, assign to table, verify on `GET .../tags` |
| Views | `go test ./internal/service/... -run TestView` | Create view, query it via `/v1/query` |
| Info Schema | `go test ./internal/engine/... -run TestInfoSchema` | `POST /v1/query` with `SELECT * FROM information_schema.tables` |

---

## Part 3: Integration Tests for New Features

### Overview

Create integration tests for all 5 new HTTP features (Tags, Views, Lineage, Query History, Search) in `test/integration/`. Tests follow existing patterns: `//go:build integration`, sequential step tests, `doRequest`/`decodeJSON` helpers, `map[string]interface{}` response parsing, `testify` assertions.

### Step 0: Update `test/integration/helpers_test.go`

Wire the 5 new services into `setupHTTPServer` and `setupIntegrationServer` so they're no longer nil.

**Changes:**

1. **Build `authSvc` unconditionally** (move out of `WithDuckLake` block) -- ViewService needs it even without DuckLake. All its dependencies are already built unconditionally.

2. **Create 5 new repositories after existing repos** (both non-DuckLake and DuckLake branches):
   ```go
   tagRepo          := repository.NewTagRepo(metaDB)
   lineageRepo      := repository.NewLineageRepo(metaDB)
   searchRepo       := repository.NewSearchRepo(metaDB)
   queryHistoryRepo := repository.NewQueryHistoryRepo(metaDB)
   viewRepo         := repository.NewViewRepo(metaDB)
   ```

3. **Create 5 new services:**
   ```go
   tagSvc          := service.NewTagService(tagRepo, auditRepo)
   lineageSvc      := service.NewLineageService(lineageRepo)
   searchSvc       := service.NewSearchService(searchRepo)
   queryHistorySvc := service.NewQueryHistoryService(queryHistoryRepo)
   ```
   For `viewSvc`: needs `catalogRepo`, `authSvc`, `auditRepo`. Build `catalogRepo := repository.NewCatalogRepo(metaDB, nil)` (duckDB=nil is safe -- `GetSchema` only reads `ducklake_schema` from metaDB). Then:
   ```go
   viewSvc := service.NewViewService(viewRepo, catalogRepo, authSvc, auditRepo)
   ```
   In the `WithDuckLake` branch, rebuild these repos/services on the replaced `metaDB`, and pass `duckDB` to `NewCatalogRepo` instead of nil.

4. **Replace nil placeholders in `NewHandler` call:**
   ```go
   handler := api.NewHandler(
       querySvc, principalSvc, groupSvc, grantSvc,
       rowFilterSvc, columnMaskSvc, introspectionSvc, auditSvc,
       manifestSvc, catalogSvc,
       queryHistorySvc, lineageSvc, searchSvc, tagSvc, viewSvc,
   )
   ```

5. **Same changes in `setupIntegrationServer`** (already has `authSvc` built).

---

### Step 1: `test/integration/tags_http_test.go` (NEW)

**Setup:** `httpTestOpts{SeedDuckLakeMetadata: true}` -- tag assignments reference `securable_id=1` (titanic table).

#### `TestHTTP_TagCRUD`

Sequential step test. All requests use `env.Keys.Admin`.

| # | Step | Method | Path | Body | Assert |
|---|------|--------|------|------|--------|
| 1 | `create_tag_with_value` | POST | `/v1/tags` | `{key:"env", value:"production"}` | 201, has `id`, `key="env"`, `value="production"`, `created_by` non-empty |
| 2 | `create_tag_no_value` | POST | `/v1/tags` | `{key:"pii"}` | 201, `value` is null |
| 3 | `create_tag_duplicate_409` | POST | `/v1/tags` | `{key:"env", value:"production"}` | 409 |
| 4 | `list_tags` | GET | `/v1/tags` | — | 200, `data` has >= 2 items |
| 5 | `assign_tag_to_table` | POST | `/v1/tags/{tagId}/assignments` | `{securable_type:"table", securable_id:1}` | 201, has `id`, `tag_id`, `securable_type="table"`, `securable_id=1` |
| 6 | `assign_tag_to_column` | POST | `/v1/tags/{tagId}/assignments` | `{securable_type:"table", securable_id:1, column_name:"Name"}` | 201 |
| 7 | `assign_tag_duplicate_409` | POST | `/v1/tags/{tagId}/assignments` | same as step 5 | 409 |
| 8 | `unassign_tag` | DELETE | `/v1/tag-assignments/{assignmentId}` | — | 204 |
| 9 | `delete_tag` | DELETE | `/v1/tags/{tagId}` | — | 204 |
| 10 | `delete_tag_not_found` | DELETE | `/v1/tags/{tagId}` | — | 404 |
| 11 | `verify_audit_logs` | GET | `/v1/audit-logs` | — | Has `CREATE_TAG`, `ASSIGN_TAG`, `UNASSIGN_TAG`, `DELETE_TAG` entries |

#### `TestHTTP_TagAnyUserCanManage`

Verify non-admin (analyst) can create and delete tags.

| # | Step | Key | Assert |
|---|------|-----|--------|
| 1 | `analyst_creates_tag` | `env.Keys.Analyst` | 201 |
| 2 | `analyst_deletes_tag` | `env.Keys.Analyst` | 204 |

---

### Step 2: `test/integration/views_http_test.go` (NEW)

**Setup:** `httpTestOpts{SeedDuckLakeMetadata: true}` -- needs `ducklake_schema` with "main" (schema_id=0).

#### `TestHTTP_ViewCRUD`

Sequential step test. Uses `env.Keys.Admin` (has `ALL_PRIVILEGES` on catalog, which includes `CREATE_TABLE`).

| # | Step | Method | Path | Body | Assert |
|---|------|--------|------|------|--------|
| 1 | `create_view` | POST | `/v1/catalog/schemas/main/views` | `{name:"v_test", view_definition:"SELECT 1", comment:"test view"}` | 201, `name="v_test"`, `schema_name="main"`, `view_definition`, `comment` |
| 2 | `create_duplicate_409` | POST | `/v1/catalog/schemas/main/views` | same name | 409 |
| 3 | `list_views` | GET | `/v1/catalog/schemas/main/views` | — | 200, `data` contains `v_test` |
| 4 | `get_view` | GET | `/v1/catalog/schemas/main/views/v_test` | — | 200, fields match |
| 5 | `get_view_not_found` | GET | `/v1/catalog/schemas/main/views/nonexistent` | — | 404 |
| 6 | `drop_view` | DELETE | `/v1/catalog/schemas/main/views/v_test` | — | 204 |
| 7 | `drop_view_not_found` | DELETE | `/v1/catalog/schemas/main/views/v_test` | — | 404 |
| 8 | `verify_audit_logs` | GET | `/v1/audit-logs` | — | Has `CREATE_VIEW`, `DROP_VIEW` entries |

#### `TestHTTP_ViewAuthZ`

Verify analyst (USAGE+SELECT only, no CREATE_TABLE) gets 403 on create/drop but can list/get.

| # | Step | Key | Assert |
|---|------|-----|--------|
| 1 | `admin_creates_view` | Admin | 201 (setup) |
| 2 | `analyst_create_403` | Analyst | POST → 403 |
| 3 | `analyst_list_200` | Analyst | GET list → 200 |
| 4 | `analyst_get_200` | Analyst | GET single → 200 |
| 5 | `analyst_drop_403` | Analyst | DELETE → 403 |

#### `TestHTTP_ViewSchemaNotFound`

| # | Step | Assert |
|---|------|--------|
| 1 | `list_bad_schema` | `GET /v1/catalog/schemas/nonexistent/views` → 404 |
| 2 | `create_bad_schema` | `POST /v1/catalog/schemas/nonexistent/views` → 404 or 400 |

---

### Step 3: `test/integration/lineage_http_test.go` (NEW)

**Setup:** `httpTestOpts{}` -- only needs `lineage_edges` table (created by migrations).

**Data seeding:** Insert edges directly via `repository.NewLineageRepo(env.MetaDB).InsertEdge(ctx, &domain.LineageEdge{...})`.

Seed graph:
```
main.orders ──READ──→ main.revenue_summary ──READ──→ main.monthly_report
main.customers ──READ──→ main.revenue_summary
```

#### `TestHTTP_LineageEndpoints`

| # | Step | Method | Path | Assert |
|---|------|--------|------|--------|
| 1 | `seed_edges` | — | — | Insert 3 edges via repo |
| 2 | `full_lineage` | GET | `/v1/lineage/tables/main/revenue_summary` | 200, `table_name="main.revenue_summary"`, `upstream` has 2, `downstream` has 1 |
| 3 | `upstream_only` | GET | `/v1/lineage/tables/main/revenue_summary/upstream` | 200, `data` has 2 edges, `edge_type="READ"` |
| 4 | `downstream_only` | GET | `/v1/lineage/tables/main/revenue_summary/downstream` | 200, `data` has 1 edge |
| 5 | `empty_lineage` | GET | `/v1/lineage/tables/main/nonexistent` | 200, `upstream` and `downstream` empty |
| 6 | `leaf_node_no_downstream` | GET | `/v1/lineage/tables/main/monthly_report/downstream` | 200, `data` empty |

#### `TestHTTP_LineageAnyUserCanQuery`

Seed edges, query with `env.Keys.Analyst` → 200.

---

### Step 4: `test/integration/query_history_http_test.go` (NEW)

**Setup:** `httpTestOpts{}` -- only needs `audit_log` table.

**Data seeding:** Insert `audit_log` entries with `action='QUERY'` via `dbstore.New(env.MetaDB).InsertAuditLog(ctx, ...)`.

Seed 3+ entries:
```
Entry 1: principal="admin_user", action="QUERY", status="ALLOWED", statement_type="SELECT", original_sql="SELECT * FROM t1", rows_returned=10
Entry 2: principal="analyst1", action="QUERY", status="ALLOWED", statement_type="SELECT", original_sql="SELECT * FROM t2", rows_returned=5
Entry 3: principal="admin_user", action="QUERY", status="DENIED", original_sql="SELECT * FROM secret"
```

#### `TestHTTP_QueryHistoryList`

| # | Step | Method | Path | Assert |
|---|------|--------|------|--------|
| 1 | `seed_entries` | — | — | Insert 3 audit entries |
| 2 | `list_all` | GET | `/v1/query-history` | 200, `data` >= 3 items, each has `principal_name`, `status` |
| 3 | `filter_by_principal` | GET | `/v1/query-history?principal_name=admin_user` | 200, all results have `principal_name="admin_user"` |
| 4 | `filter_by_status` | GET | `/v1/query-history?status=DENIED` | 200, all results have `status="DENIED"` |
| 5 | `pagination` | GET | `/v1/query-history?max_results=1` | 200, `data` has 1 item, `next_page_token` non-nil; follow to next page, get different entry |

#### `TestHTTP_QueryHistoryAnyUserCanList`

Call with `env.Keys.Analyst` → 200.

---

### Step 5: `test/integration/search_http_test.go` (NEW)

**Setup:** `httpTestOpts{SeedDuckLakeMetadata: true}` -- search queries `ducklake_schema`, `ducklake_table`, `ducklake_column`.

Seeded data: schema "main", table "titanic", 12 columns (PassengerId, Survived, ...).

#### `TestHTTP_SearchCatalog`

| # | Step | Method | Path | Assert |
|---|------|--------|------|--------|
| 1 | `search_table` | GET | `/v1/search?query=titanic` | 200, `data` contains result with `type="table"`, `name="titanic"` |
| 2 | `search_column` | GET | `/v1/search?query=Survived` | 200, `data` contains result with `type="column"` |
| 3 | `search_type_filter` | GET | `/v1/search?query=main&type=schema` | 200, all results have `type="schema"` |
| 4 | `search_no_results` | GET | `/v1/search?query=zzzznonexistent` | 200, `data` is empty or null |
| 5 | `search_pagination` | GET | `/v1/search?query=a&max_results=2` | 200, `data` has <= 2 items |

#### `TestHTTP_SearchAnyUserCanSearch`

Call with `env.Keys.Analyst` → 200.

---

### Verification

```bash
# Run just the new integration tests (requires migrations, no S3/DuckLake extensions needed):
go test -tags integration -v -timeout 120s -run "TestHTTP_Tag|TestHTTP_View|TestHTTP_Lineage|TestHTTP_QueryHistory|TestHTTP_Search" ./test/integration/...

# Run all integration tests to verify nothing broke:
task integration-test
```

### Files Summary

| File | Action | Lines (est.) |
|------|--------|-------------|
| `test/integration/helpers_test.go` | Modify | +40 (wire repos/services) |
| `test/integration/tags_http_test.go` | Create | ~180 |
| `test/integration/views_http_test.go` | Create | ~200 |
| `test/integration/lineage_http_test.go` | Create | ~150 |
| `test/integration/query_history_http_test.go` | Create | ~140 |
| `test/integration/search_http_test.go` | Create | ~120 |

---

## Part 4: Unit Tests for Feature Completeness

### Coverage Gap Summary

Integration tests cover the full HTTP stack for 5 of 6 features. What's missing:

| Layer | Status | Priority |
|-------|--------|----------|
| **Service unit tests** (5 services) | MISSING | P0 — business logic (auth, principal injection, audit) |
| **InformationSchemaProvider** tests | MISSING — no tests at all (not even integration) | P0 |
| **ExtractTargetTable** tests | MISSING — pure function, zero tests | P0 |
| **Repository unit tests** (5 repos) | MISSING | P2 — skip, covered by integration tests |
| **Mapper tests** (6 functions) | MISSING | P2 — skip, trivial conversions |
| **API handler unit tests** (15 handlers) | MISSING | P2 — skip, covered by integration tests |

**Decision: Implement P0 only.** Repository/mapper/handler tests would duplicate integration test coverage with minimal extra value.

---

### File 1: `internal/service/mock_test.go` (NEW)

Shared mocks for all service tests. Follow pattern from `internal/api/mock_catalog_test.go` — function fields for configurable behavior.

```go
package service

// mockViewRepo implements domain.ViewRepository
// mockTagRepo implements domain.TagRepository
// mockLineageRepo implements domain.LineageRepository
// mockQueryHistoryRepo implements domain.QueryHistoryRepository
// mockSearchRepo implements domain.SearchRepository
// mockCatalogRepo implements domain.CatalogRepository (subset: GetSchema, ListSchemas, ListTables, ListColumns)
// mockAuthService implements domain.AuthorizationService (subset: CheckPrivilege)
// mockAuditRepo implements domain.AuditRepository (captures last entry)
```

Each mock uses `func(...)` fields so tests configure only the methods they need. Unconfigured methods panic with "unexpected call".

---

### File 2: `internal/service/views_test.go` (NEW)

Most complex service — 4 dependencies (ViewRepo, CatalogRepo, AuthService, AuditRepo), authorization checks, schema resolution, field enrichment, audit logging.

**`TestViewService_CreateView`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | Returns view with correct SchemaName, CatalogName, Owner=principal |
| `access_denied` | CheckPrivilege returns false → `*domain.AccessDeniedError` |
| `auth_check_error` | CheckPrivilege returns error → error propagated with "check privilege:" |
| `schema_not_found` | GetSchema returns NotFoundError → propagated |
| `repo_create_error` | repo.Create returns error → propagated |
| `sets_owner_from_principal` | Capture arg to repo.Create, verify Owner == principal name |
| `audit_logged` | Verify audit entry with action="CREATE_VIEW" |

**`TestViewService_GetView`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | Enriches result with SchemaName, CatalogName |
| `schema_not_found` | Error propagated |
| `view_not_found` | Error propagated |

**`TestViewService_ListViews`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | All views enriched with SchemaName, CatalogName; total returned |
| `empty_result` | Empty slice, total=0 |
| `schema_not_found` | Error propagated |

**`TestViewService_DeleteView`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | No error; audit logged with "DROP_VIEW" |
| `access_denied` | AccessDeniedError returned |
| `schema_not_found` | Error propagated |
| `repo_delete_error` | Error propagated |

---

### File 3: `internal/service/tags_test.go` (NEW)

8 methods, 4 have business logic (principal injection + audit).

**`TestTagService_CreateTag`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | Sets CreatedBy=principal, audit with "CREATE_TAG" |
| `repo_error` | Error propagated, no audit |

**`TestTagService_DeleteTag`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | No error, audit with "DELETE_TAG" |
| `repo_error` | Error propagated, no audit |

**`TestTagService_AssignTag`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | Sets AssignedBy=principal, audit with "ASSIGN_TAG" |
| `repo_error` | Error propagated, no audit |

**`TestTagService_UnassignTag`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | No error, audit with "UNASSIGN_TAG" |
| `repo_error` | Error propagated, no audit |

**`TestTagService_GetTag`** — happy path + not found error

**`TestTagService_ListTags`** — happy path + empty

**`TestTagService_ListTagsForSecurable`** — delegation test

**`TestTagService_ListAssignmentsForTag`** — delegation test

---

### File 4: `internal/service/lineage_test.go` (NEW)

Key test: `GetFullLineage` orchestration logic.

**`TestLineageService_GetFullLineage`**
| Subtest | What's tested |
|---------|--------------|
| `happy_path` | Combines upstream + downstream into LineageNode |
| `upstream_error` | Error returned, downstream not called |
| `downstream_error` | Upstream succeeds, downstream error propagated |
| `both_empty` | Returns node with empty slices |

**`TestLineageService_InsertEdge`** — delegation + error propagation
**`TestLineageService_GetUpstream`** — delegation + error propagation
**`TestLineageService_GetDownstream`** — delegation + error propagation

---

### File 5: `internal/service/query_history_test.go` (NEW)

Thin pass-through.

**`TestQueryHistoryService_List`**
| Subtest | What's tested |
|---------|--------------|
| `delegates_to_repo` | Filter passed through, results returned |
| `repo_error` | Error propagated |

---

### File 6: `internal/service/search_test.go` (NEW)

Thin pass-through with pagination translation.

**`TestSearchService_Search`**
| Subtest | What's tested |
|---------|--------------|
| `delegates_with_pagination` | page.Limit() and page.Offset() correctly translated |
| `repo_error` | Error propagated |

---

### File 7: `internal/engine/information_schema_test.go` (NEW)

Uses mock CatalogRepository (only needs ListSchemas, ListTables, ListColumns).

**`TestIsInformationSchemaQuery`** (pure function, no mocks)
| Subtest | Input | Expected |
|---------|-------|----------|
| `schemata` | `"SELECT * FROM information_schema.schemata"` | true |
| `tables_uppercase` | `"SELECT * FROM INFORMATION_SCHEMA.TABLES"` | true |
| `columns_mixed` | `"... information_schema.columns ..."` | true |
| `no_match` | `"SELECT * FROM users"` | false |
| `partial_no_dot` | `"SELECT * FROM information_schema_foo"` | false |
| `empty_string` | `""` | false |

**`TestBuildSchemataRows`**
| Subtest | What's tested |
|---------|--------------|
| `two_schemas` | 2 rows returned, correct columns |
| `empty` | 0 rows, columns still returned |
| `list_error` | Error propagated |

**`TestBuildTablesRows`**
| Subtest | What's tested |
|---------|--------------|
| `tables_across_schemas` | Correct catalog/schema/table names |
| `schema_list_error` | Error propagated |
| `table_list_error_continues` | Skips failing schema (matches `continue` in source) |

**`TestBuildColumnsRows`**
| Subtest | What's tested |
|---------|--------------|
| `columns_present` | Correct ordinal_position and data_type |
| `column_list_error_continues` | Skips failing table |

Note: `HandleQuery` requires a real `*sql.DB` (DuckDB for temp tables). Test `Build*Rows` thoroughly with mocks. `HandleQuery` is covered by the fact that `Build*Rows` + temp table creation is straightforward string manipulation — defer to a future integration test if needed.

---

### File 8: `internal/sqlrewrite/sqlrewrite_test.go` (MODIFY — add tests)

Add `TestExtractTargetTable` to existing file.

| Subtest | Input | Expected |
|---------|-------|----------|
| `insert` | `"INSERT INTO orders (id) VALUES (1)"` | `"orders"` |
| `update` | `"UPDATE users SET name = 'x' WHERE id = 1"` | `"users"` |
| `delete` | `"DELETE FROM logs WHERE ts < '2024-01-01'"` | `"logs"` |
| `select_returns_empty` | `"SELECT * FROM titanic"` | `""` |
| `create_table_returns_empty` | `"CREATE TABLE foo (id INT)"` | `""` |
| `empty_sql` | `""` | `""` |
| `insert_with_schema_prefix` | `"INSERT INTO main.orders (id) VALUES (1)"` | `"orders"` |
| `update_with_subquery` | `"UPDATE orders SET total = (SELECT SUM(amount) FROM items)"` | `"orders"` |

---

### Implementation Order

1. `internal/service/mock_test.go` — shared mocks (unblocks all service tests)
2. `internal/service/views_test.go` — highest complexity
3. `internal/service/tags_test.go` — principal injection + audit
4. `internal/service/lineage_test.go` — orchestration logic
5. `internal/sqlrewrite/sqlrewrite_test.go` — add ExtractTargetTable cases (quick win)
6. `internal/engine/information_schema_test.go` — needs its own mock catalog
7. `internal/service/query_history_test.go` — thin delegation
8. `internal/service/search_test.go` — thin delegation

### Files Summary

| File | Action | Est. subtests |
|------|--------|---------------|
| `internal/service/mock_test.go` | Create | — (shared infrastructure) |
| `internal/service/views_test.go` | Create | ~14 |
| `internal/service/tags_test.go` | Create | ~14 |
| `internal/service/lineage_test.go` | Create | ~9 |
| `internal/service/query_history_test.go` | Create | ~2 |
| `internal/service/search_test.go` | Create | ~2 |
| `internal/engine/information_schema_test.go` | Create | ~12 |
| `internal/sqlrewrite/sqlrewrite_test.go` | Modify | +8 |
| **Total** | | **~61 new test cases** |

### Verification

```bash
# Run all unit tests (includes new ones):
task test

# Run just the new service tests:
go test -race -run "TestViewService|TestTagService|TestLineageService|TestQueryHistoryService|TestSearchService" ./internal/service/...

# Run information schema tests:
go test -race -run "TestIsInformationSchemaQuery|TestBuild" ./internal/engine/...

# Run ExtractTargetTable tests:
go test -race -run "TestExtractTargetTable" ./internal/sqlrewrite/...

# Verify nothing broke:
task build && task vet && task test
```

# Plan: DuckLake Ingestion API via `ducklake_add_data_files`

## Overview

Add a Parquet ingestion API that uses DuckLake's `ducklake_add_data_files()` function as the core mechanism. Two ingestion patterns:

1. **Presigned upload** — client gets a PUT URL, uploads Parquet to S3, then commits
2. **Server-side load** — client provides existing S3 paths/globs, server registers them

The `ducklake_add_data_files()` function handles all the hard work: reading Parquet metadata, validating schema compatibility, extracting column statistics, and registering files in the DuckLake catalog atomically.

---

## API Endpoints (3 new)

All under `/v1/catalog/schemas/{schemaName}/tables/{tableName}/ingestion/`:

### 1. `POST .../upload-url` — Request presigned PUT URL

```
Request:  { "filename": "orders_jan.parquet" }  // optional hint
Response: { "upload_url": "https://...", "s3_key": "lake_data/.../uuid_orders_jan.parquet", "expires_at": "..." }
```
- Checks INSERT privilege on the table
- Generates S3 key: `lake_data/<schema>/<table>/uploads/<uuid>[_<filename>].parquet`
- Returns presigned PUT URL (1 hour expiry)

### 2. `POST .../commit` — Register uploaded file(s) in DuckLake

```
Request:  { "s3_keys": ["lake_data/.../uuid.parquet"], "options": { "allow_missing_columns": false, "ignore_extra_columns": false } }
Response: { "files_registered": 3, "files_skipped": 0, "schema": "main", "table": "orders" }
```
- Checks INSERT privilege
- Builds full `s3://` URIs from the keys
- Executes `CALL ducklake_add_data_files('lake', 'orders', ['s3://...'], schema => 'main', ...)`
- DuckLake validates schema, extracts stats, registers files atomically

### 3. `POST .../load` — Register existing S3 files

```
Request:  { "paths": ["s3://bucket/data/*.parquet"], "options": { "allow_missing_columns": true } }
Response: { "files_registered": 10, "files_skipped": 1, "schema": "main", "table": "events" }
```
- Same as commit but accepts arbitrary S3 paths or globs
- Paths without `s3://` prefix are treated as relative to the lake data path

---

## Architecture

```
API Handler (handler.go)
    │
    ▼
IngestionService (new: service/ingestion.go)
    ├── S3Presigner.PresignPutObject()     — for upload-url
    ├── AuthorizationService.CheckPrivilege() — INSERT check
    ├── duckDB.QueryContext()               — CALL ducklake_add_data_files(...)
    ├── metaDB.QueryRow()                   — read data_path from ducklake_metadata
    └── AuditRepository.Insert()            — audit logging
```

The CALL statement is executed **directly on the `*sql.DB` DuckDB connection**, bypassing SecureEngine. This matches the existing pattern where `CatalogRepository` executes DDL directly on DuckDB. The SecureEngine's SQL parser (`pg_query_go`) cannot parse DuckDB `CALL` statements, and authorization is already handled at the service layer.

---

## `ducklake_add_data_files()` — How It Works

The function signature (from source):
```sql
CALL ducklake_add_data_files(
    'lake',                           -- catalog name (attached DB)
    'my_table',                       -- table name
    ['s3://bucket/file.parquet'],     -- file path(s) or glob(s)
    schema => 'main',                 -- optional: schema name
    allow_missing => false,           -- optional: allow missing columns
    ignore_extra_columns => false,    -- optional: ignore extra columns
    hive_partitioning => true         -- optional: hive partition detection
);
```

Internally it:
1. Reads Parquet metadata via DuckDB's `parquet_full_metadata()` — **no data is read**, only footer
2. Validates column names (case-insensitive match) and types (with widening rules: TINYINT→BIGINT, FLOAT→DOUBLE, etc.)
3. Extracts per-column statistics (min/max/null_count/value_count/column_size)
4. Handles hive partitioning (auto-detected from file paths)
5. Creates column name mappings (`ducklake_column_mapping` + `ducklake_name_mapping`) for schema evolution
6. Registers files in `ducklake_data_file`, `ducklake_file_column_stats`, `ducklake_file_partition_value`
7. Empty files (0 rows) are silently skipped
8. All within a DuckLake transaction with snapshot conflict detection + retry

---

## Files to Create

### `internal/domain/ingestion.go`
```go
type IngestionOptions struct {
    AllowMissingColumns bool
    IgnoreExtraColumns  bool
}

type UploadURLResult struct {
    UploadURL string
    S3Key     string
    ExpiresAt time.Time
}

type IngestionResult struct {
    FilesRegistered int
    FilesSkipped    int
    Table           string
    Schema          string
}
```

### `internal/service/ingestion.go`
New `IngestionService` with:
- `RequestUploadURL(ctx, schemaName, tableName, filename *string) (*UploadURLResult, error)`
- `CommitIngestion(ctx, schemaName, tableName, s3Keys []string, opts IngestionOptions) (*IngestionResult, error)`
- `LoadExternalFiles(ctx, schemaName, tableName, paths []string, opts IngestionOptions) (*IngestionResult, error)`

Dependencies: `duckDB *sql.DB`, `metaDB *sql.DB`, `authSvc`, `presigner`, `auditRepo`, `catalogName string`, `bucket string`

Core helper `execAddDataFiles()` builds and executes:
```sql
CALL ducklake_add_data_files('lake', 'tableName', ['s3://...', ...],
    schema => 'schemaName', allow_missing => true, ignore_extra_columns => false)
```

### `internal/service/ingestion_test.go`
Table-driven tests covering:
- Happy path: upload URL generation, commit, load
- Auth denied (no INSERT privilege)
- Table not found
- Schema mismatch (DuckDB error classification)
- Empty s3_keys list validation

---

## Files to Modify

### `internal/service/presigner.go`
Add `PresignPutObject(ctx, bucket, key string, expiry time.Duration) (string, error)`:
```go
func (p *S3Presigner) PresignPutObject(ctx context.Context, bucket, key string, expiry time.Duration) (string, error) {
    result, err := p.presignClient.PresignPutObject(ctx,
        &s3.PutObjectInput{
            Bucket:      aws.String(bucket),
            Key:         aws.String(key),
            ContentType: aws.String("application/octet-stream"),
        },
        s3.WithPresignExpires(expiry),
    )
    if err != nil {
        return "", fmt.Errorf("presign PutObject for %q/%q: %w", bucket, key, err)
    }
    return result.URL, nil
}
```

Add `Bucket() string` accessor.

### `internal/api/openapi.yaml`
Add 3 new paths under `/catalog/schemas/{schemaName}/tables/{tableName}/ingestion/`:
- `upload-url` (POST) → `requestUploadUrl`
- `commit` (POST) → `commitIngestion`
- `load` (POST) → `loadExternalFiles`

Add 6 new schema types:
- `UploadUrlRequest`, `UploadUrlResponse`
- `CommitIngestionRequest`, `LoadExternalRequest`
- `IngestionOptions`, `IngestionResult`

### `internal/api/handler.go`
- Add `ingestion *service.IngestionService` field to `APIHandler`
- Update `NewHandler` constructor signature
- Implement 3 new handler methods following existing patterns:
  - `RequestUploadUrl()` — extract principal, call service, return typed response
  - `CommitIngestion()` — same pattern
  - `LoadExternalFiles()` — same pattern

### `cmd/server/main.go`
- Create `IngestionService` in composition root (after presigner):
```go
var ingestionSvc *service.IngestionService
if cfgErr == nil && presigner != nil {
    ingestionSvc = service.NewIngestionService(
        duckDB, metaDB, cat, presigner, auditRepo, "lake", cfg.S3Bucket,
    )
    log.Println("Ingestion service enabled")
}
```
- Pass `ingestionSvc` to `api.NewHandler()`

### Regenerated files (via `task generate-api`)
- `internal/api/types.gen.go` — new generated types
- `internal/api/server.gen.go` — new interface methods + route registrations

---

## Files NOT Modified

| File | Why |
|------|-----|
| `internal/engine/engine.go` | CALL bypasses engine; executed directly on `*sql.DB` |
| `internal/sqlrewrite/` | No new statement types |
| `internal/domain/repository.go` | No new repo interfaces; ingestion uses DuckDB directly |
| `internal/domain/errors.go` | Existing error types suffice |
| `internal/db/queries/*.sql` | No new sqlc queries |
| `internal/config/config.go` | Uses existing S3 config |

---

## Error Handling

DuckDB errors from `ducklake_add_data_files()` are classified into domain errors:

| DuckDB Error | Domain Error | HTTP |
|-------------|-------------|------|
| Column type mismatch | `ValidationError` | 400 |
| Column not found in table | `ValidationError` | 400 |
| No files found at path | `ValidationError` | 400 |
| Could not read file | `ValidationError` | 400 |
| Table does not exist | `NotFoundError` | 404 |
| Other errors | `ValidationError` | 400 |

---

## Edge Cases

**Upload never completes**: Presigned URL expires (1h). Orphaned files in `uploads/` don't affect correctness — they consume storage but aren't in the catalog. Future cleanup job can scan for orphans.

**Schema mismatch**: DuckLake validates column names/types from Parquet footer. Clear error messages returned. Client can use `allow_missing_columns`/`ignore_extra_columns` for flexibility.

**Concurrent additions**: DuckLake handles this — append-only file additions to the same table don't conflict. Each commit gets a new snapshot ID atomically.

**Path traversal**: For `commit`, keys are server-generated (from `upload-url`). For `load`, paths are user-provided but bounded by DuckDB's S3 secret scope.

---

## Verification

1. **Unit tests**: `internal/service/ingestion_test.go` with mocked DuckDB
2. **Integration test**: Create table via catalog API → upload Parquet → commit → query via `/v1/query` and verify data is visible
3. **Manual test**:
   ```bash
   # Create table
   curl -X POST /v1/catalog/schemas/main/tables -d '{"name":"events","columns":[...]}'
   
   # Get upload URL
   curl -X POST /v1/catalog/schemas/main/tables/events/ingestion/upload-url \
     -d '{"filename":"events.parquet"}'
   
   # Upload file
   curl -X PUT "<presigned_url>" --upload-file events.parquet
   
   # Commit
   curl -X POST /v1/catalog/schemas/main/tables/events/ingestion/commit \
     -d '{"s3_keys":["lake_data/main/events/uploads/uuid_events.parquet"]}'
   
   # Verify
   curl -X POST /v1/query -d '{"sql":"SELECT count(*) FROM lake.main.events"}'
   ```
4. **Build**: `task build && task test`
